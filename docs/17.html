
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <title>十七、使用自编码器和 GAN 做表征学习和生成式学习 · HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 5.1.4">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/@honkit/honkit-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, user-scalable=yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="18.html" />
    
    
    <link rel="prev" href="16.html" />
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="0.html">
            
                <a href="0.html">
            
                    
                    零、前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="1.html">
            
                <a href="1.html">
            
                    
                    一、机器学习概览
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="2.html">
            
                <a href="2.html">
            
                    
                    二、端到端的机器学习项目
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="3.html">
            
                <a href="3.html">
            
                    
                    三、分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="4.html">
            
                <a href="4.html">
            
                    
                    四、训练模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="5.html">
            
                <a href="5.html">
            
                    
                    五、支持向量机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="6.html">
            
                <a href="6.html">
            
                    
                    六、决策树
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="7.html">
            
                <a href="7.html">
            
                    
                    七、集成学习和随机森林
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="8.html">
            
                <a href="8.html">
            
                    
                    八、降维
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="10.html">
            
                <a href="10.html">
            
                    
                    十、使用 Keras 搭建人工神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12" data-path="11.html">
            
                <a href="11.html">
            
                    
                    十一、训练深度神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.13" data-path="12.html">
            
                <a href="12.html">
            
                    
                    十二、使用 TensorFlow 自定义模型并训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14" data-path="13.html">
            
                <a href="13.html">
            
                    
                    十三、使用 TensorFlow 加载和预处理数据
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15" data-path="14.html">
            
                <a href="14.html">
            
                    
                    十四、使用卷积神经网络实现深度计算机视觉
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16" data-path="15.html">
            
                <a href="15.html">
            
                    
                    十五、使用 RNN 和 CNN 处理序列
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17" data-path="16.html">
            
                <a href="16.html">
            
                    
                    十六、使用 RNN 和注意力机制进行自然语言处理
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.18" data-path="17.html">
            
                <a href="17.html">
            
                    
                    十七、使用自编码器和 GAN 做表征学习和生成式学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.19" data-path="18.html">
            
                <a href="18.html">
            
                    
                    十八、强化学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.20" data-path="19.html">
            
                <a href="19.html">
            
                    
                    十九、规模化训练和部署 TensorFlow 模型
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Published with HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >十七、使用自编码器和 GAN 做表征学习和生成式学习</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="十七、使用自编码器和-gan-做表征学习和生成式学习">十七、使用自编码器和 GAN 做表征学习和生成式学习</h1>
<blockquote>
<p>译者：<a href="https://www.jianshu.com/u/130f76596b02" target="_blank">@SeanCheney</a></p>
</blockquote>
<p>自编码器是能够在无监督（即，训练集是未标记）的情况下学习输入数据的紧密表征（叫做潜在表征或编码）的人工神经网络。这些编码通常具有比输入数据低得多的维度，使得自编码器对降维有用（参见第 8 章）。自编码器还可以作为强大的特征检测器，它们可以用于无监督的深度神经网络预训练（正如我们在第 11 章中讨论过的）。最后，一些自编码器是生成式模型：他们能够随机生成与训练数据非常相似的新数据。例如，您可以在脸图片上训练自编码器，然后可以生成新脸。但是生成出来的图片通常是模糊且不够真实。</p>
<p>相反，用对抗生成网络（GAN）生成的人脸可以非常逼真，甚至让人认为他们是真实存在的人。你可以去<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fthispersondoesnotexist.com%2F" target="_blank">这个网址</a>，这是用 StyleGAN 生成的人脸，自己判断一下（还可以去<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fthisrentaldoesnotexist.com%2F" target="_blank">这里</a>，看看 GAN 生成的卧室图片），GAN 现在广泛用于超清图片涂色，图片编辑，将草图变为照片，增强数据集，生成其它类型的数据（比如文本、音频、时间序列），找出其它模型的缺点并强化，等等。</p>
<p>自编码器和 GAN 都是无监督的，都可以学习紧密表征，都可以用作生成模型，有许多相似的应用，但原理非常不同：</p>
<ul>
<li><p>自编码器是通过学习，将输入复制到输出。听起来很简单，但内部结构会使其相当困难。例如，你可以限制潜在表征的大小，或者可以给输入添加噪音，训练模型恢复原始输入。这些限制组织自编码器直接将输入复制到输出，可以强迫模型学习数据的高效表征。总而言之，编码是自编码器在一些限制下学习恒等函数的副产品。</p>
</li>
<li><p>GAN 包括两个神经网络：一个生成器尝试生成和训练数据相似的数据，一个判别器来区分真实数据和假数据。特别之处在于，生成器和判别器在训练过程中彼此竞争：生成器就像一个制造伪钞的罪犯，而判别器就像警察一样，要把真钱挑出来。对抗训练（训练竞争神经网络），被认为是近几年的一大进展。在 2016 年，Yann LeCun 甚至说 GAN 是过去 10 年机器学习领域最有趣的发明。</p>
</li>
</ul>
<p>本章中，我们先探究自编码器的工作原理开始，如何做降维、特征提取、无监督预训练将、如何用作生成式模型。然后过渡到 GAN。先用 GAN 生成假图片，可以看到训练很困难。会讨论对抗训练的主要难点，以及一些解决方法。先从自编码器开始。</p>
<h2 id="有效的数据表征">有效的数据表征</h2>
<p>以下哪一个数字序列更容易记忆？</p>
<ul>
<li>40, 27, 25, 36, 81, 57, 10, 73, 19, 68</li>
<li>50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14</li>
</ul>
<p>乍一看，第一个序列似乎应该更容易，因为它要短得多。 但是，如果仔细观察第二个序列，就会发现它是从 50 到 14 的偶数。一旦你注意到这个规律，第二个序列比第一个更容易记忆，因为你只需要记住规律就成，开始的数字和结尾的数字。请注意，如果您可以快速轻松地记住非常长的序列，则不会在意第二个序列中存在的规律。 只要记住每一个数字，就够了。 事实上，很难记住长序列，因此识别规律非常有用，并且希望能够澄清为什么在训练过程中限制自编码器会促使它发现并利用数据中的规律。</p>
<p>记忆、感知和模式匹配之间的关系在 20 世纪 70 年代早期由 William Chase 和 Herbert Simon 研究。 他们观察到，专业棋手能够通过观看棋盘 5 秒钟就能记住所有棋子的位置，这是大多数人认为不可能完成的任务。 然而，只有当这些棋子被放置在现实位置（来自实际比赛）时才是这种情况，而不是随机放置棋子。 国际象棋专业棋手没有比你更好的记忆，他们只是更容易看到国际象棋的规律，这要归功于他们的比赛经验。 观察规律有助于他们有效地存储信息。</p>
<p>就像这个记忆实验中的象棋棋手一样，一个自编码器会查看输入信息，将它们转换为高效的潜在表征，然后输出一些（希望）看起来非常接近输入的东西。 自编码器总是由两部分组成：将输入转换为潜在表征的编码器（或识别网络），然后是将潜在表征转换为输出的解码器（或生成网络）（见图 17-1）。</p>
<p><img src="img/133f15a30a9de1a24f0a24873dd09824.png" alt=""></img></p>
<p>图 17-1 记忆象棋试验（左）和一个简单的自编码器（右）</p>
<p>如你所见，自编码器通常具有与多层感知器（MLP，请参阅第 10 章）相同的体系结构，但输出层中的神经元数量必须等于输入数量。 在这个例子中，只有一个由两个神经元（编码器）组成的隐藏层和一个由三个神经元（解码器）组成的输出层。由于自编码器试图重构输入，所以输出通常被称为重建，并且损失函数包含重建损失，当重建与输入不同时，重建损失会对模型进行惩罚。</p>
<p>由于内部表征具有比输入数据更低的维度（它是 2D 而不是 3D），所以自编码器被认为是不完整的。 不完整的自编码器不能简单地将其输入复制到编码，但它必须找到一种方法来输出其输入的副本。 它被迫学习输入数据中最重要的特征（并删除不重要的特征）。</p>
<p>我们来看看如何实现一个非常简单的不完整的自编码器，以降低维度。</p>
<h2 id="用不完整的线性自编码器来做-pca">用不完整的线性自编码器来做 PCA</h2>
<p>如果自编码器仅使用线性激活并且损失函数是均方误差（MSE），最终其实是做了主成分分析（参见第 8 章）。</p>
<p>以下代码创建了一个简单的线性自编码器，以在 3D 数据集上执行 PCA，并将其投影到 2D：</p>
<pre><code class="lang-py"><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras

encoder = keras.models.Sequential([keras.layers.Dense(<span class="hljs-number">2</span>, input_shape=[<span class="hljs-number">3</span>])])
decoder = keras.models.Sequential([keras.layers.Dense(<span class="hljs-number">3</span>, input_shape=[<span class="hljs-number">2</span>])])
autoencoder = keras.models.Sequential([encoder, decoder])

autoencoder.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">"mse"</span>, optimizer=keras.optimizers.SGD(lr=<span class="hljs-number">0.1</span>))
</code></pre>
<p>这段代码与我们在前面章节中创建的所有 MLP 没有什么大不同。只有以下几点要注意：</p>
<ul>
<li><p>自编码器由两部分组成：编码器和解码器。两者都是常规的<code>Sequential</code>模型，每个含有一个紧密层，自编码器是一个编码器和解码器连起来的<code>Sequential</code>模型（模型可以用作其它模型中的层）。</p>
</li>
<li><p>自编码器的输出等于输入。</p>
</li>
<li><p>简单 PCA 不需要激活函数（即，所有神经元是线性的），且损失函数是 MSE。后面会看到更复杂的自编码器。</p>
</li>
</ul>
<p>现在用生成出来的 3D 数据集训练模型，并用模型编码数据集（即将其投影到 2D）：</p>
<pre><code class="lang-py">history = autoencoder.fit(X_train, X_train, epochs=<span class="hljs-number">20</span>)
codings = encoder.predict(X_train)
</code></pre>
<p>注意，<code>X_train</code>既用来做输入，也用来做目标。图 17-2 显示了原始 3D 数据集（左侧）和自编码器隐藏层的输出（即编码层，右侧）。 可以看到，自编码器找到了投影数据的最佳二维平面，保留了数据的尽可能多的差异（就像 PCA 一样）。</p>
<p><img src="img/e6301e8ec35adb6618df0c051f883dce.png" alt=""></img></p>
<p>图 17-2 用不完整的线性自编码器实现 PCA</p>
<blockquote>
<p>笔记：可以将自编码器当做某种形式的自监督学习（带有自动生成标签功能的监督学习，这个例子中标签等于输入）</p>
</blockquote>
<h2 id="栈式自编码器">栈式自编码器</h2>
<p>就像我们讨论过的其他神经网络一样，自编码器可以有多个隐藏层。 在这种情况下，它们被称为栈式自编码器（或深度自编码器）。 添加更多层有助于自编码器了解更复杂的编码。 但是，必须注意不要让自编码器功能太强大。 设想一个编码器非常强大，只需学习将每个输入映射到一个任意数字（并且解码器学习反向映射）即可。 很明显，这样的自编码器将完美地重构训练数据，但它不会在过程中学习到任何有用的数据表征（并且它不可能很好地泛化到新的实例）。</p>
<p>栈式自编码器的架构以中央隐藏层（编码层）为中心通常是对称的。 简单来说，它看起来像一个三明治。 例如，一个用于 MNIST 的自编码器（在第 3 章中介绍）可能有 784 个输入，其次是一个隐藏层，有 100 个神经元，然后是一个中央隐藏层，有 30 个神经元，然后是另一个隐藏层，有 100 个神经元，输出层有 784 个神经元。 这个栈式自编码器如图 17-3 所示。</p>
<p><img src="img/2eb643c0821b45d0728233dbf25d1e46.png" alt=""></img></p>
<p>图 17-3 栈式自编码器</p>
<h3 id="用-keras-实现栈式自编码器">用 Keras 实现栈式自编码器</h3>
<p>你可以像常规深度 MLP 一样实现栈式自编码器。 特别是，我们在第 11 章中用于训练深度网络的技术也可以应用。例如，下面的代码使用 SELU 激活函数为 Fashion MNIST 创建了一个栈式自编码器：</p>
<pre><code class="lang-py">stacked_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>]),
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">"selu"</span>),
])
stacked_decoder = keras.models.Sequential([
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>, input_shape=[<span class="hljs-number">30</span>]),
    keras.layers.Dense(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, activation=<span class="hljs-string">"sigmoid"</span>),
    keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])
])
stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])
stacked_ae.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">"binary_crossentropy"</span>,
                   optimizer=keras.optimizers.SGD(lr=<span class="hljs-number">1.5</span>))
history = stacked_ae.fit(X_train, X_train, epochs=<span class="hljs-number">10</span>,
                         validation_data=[X_valid, X_valid])
</code></pre>
<p>逐行看下这个代码：</p>
<ul>
<li><p>和之前一样，自编码器包括两个子模块：编码器和解码器。</p>
</li>
<li><p>编码器接收<code>28 × 28</code>像素的灰度图片，打平为大小等于 784 的向量，用两个紧密层来处理，两个紧密层都是用 SELU 激活函数（还可以加上 LeCun 归一初始化，但因为网络不深，效果不大）。对于每张输入图片，编码器输出的向量大小是 30。</p>
</li>
<li><p>解码器接收大小等于 30 的编码（编码器的输出），用两个紧密层来处理，最后的向量转换为<code>28 × 28</code>的数组，使解码器的输出和编码器的输入形状相同。</p>
</li>
<li><p>编译时，使用二元交叉熵损失，而不是 MSE。将重建任务当做多标签分类问题：每个像素强度表示像素应该为黑色的概率。这么界定问题（而不是当做回归问题），可以使模型收敛更快。</p>
</li>
<li><p>最后，使用<code>X_train</code>既作为输入，也作为目标，来训练模型（相似的，使用<code>X_valid</code>既作为验证的输入也作为目标）。</p>
</li>
</ul>
<h3 id="可视化重建">可视化重建</h3>
<p>确保自编码器训练得当的方式之一，是比较输入和输出：差异不应过大。画一些验证集的图片，及其重建：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_image</span>(<span class="hljs-params">image</span>):
    plt.imshow(image, cmap=<span class="hljs-string">"binary"</span>)
    plt.axis(<span class="hljs-string">"off"</span>)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">show_reconstructions</span>(<span class="hljs-params">model, n_images=<span class="hljs-number">5</span></span>):
    reconstructions = model.predict(X_valid[:n_images])
    fig = plt.figure(figsize=(n_images * <span class="hljs-number">1.5</span>, <span class="hljs-number">3</span>))
    <span class="hljs-keyword">for</span> image_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_images):
        plt.subplot(<span class="hljs-number">2</span>, n_images, <span class="hljs-number">1</span> + image_index)
        plot_image(X_valid[image_index])
        plt.subplot(<span class="hljs-number">2</span>, n_images, <span class="hljs-number">1</span> + n_images + image_index)
        plot_image(reconstructions[image_index])

show_reconstructions(stacked_ae)
</code></pre>
<p>图 17-4 展示了比较结果。</p>
<p><img src="img/57ef0ccfc6045465417fbe4bfb671695.png" alt=""></img></p>
<p>图 17-4 原始图片（上）及其重建（下）</p>
<p>可以认出重建，但图片有些失真。需要再训练模型一段时间，或使编码器和解码器更深，或使编码更大。但如果使网络太强大，就学不到数据中的规律。</p>
<h3 id="可视化-fashion-mnist-数据集">可视化 Fashion MNIST 数据集</h3>
<p>训练好栈式自编码器之后，就可以用它给数据集降维了。可视化的话，结果不像（第 8 章其它介绍的）其它降维方法那么好，但自编码器的优势是可以处理带有多个实例多个特征的大数据集。所以一个策略是利用自编码器将数据集降维到一个合理的水平，然后使用另外一个降维算法做可视化。用这个策略来可视化 Fashion MNIST。首先，使用栈式自编码器的编码器将维度降到 30，然后使用 Scikit-Learn 的 t-SNE 算法实现，将维度降到 2 并做可视化：</p>
<pre><code class="lang-py"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE

X_valid_compressed = stacked_encoder.predict(X_valid)
tsne = TSNE()
X_valid_2D = tsne.fit_transform(X_valid_compressed)
</code></pre>
<p>对数据集作图：</p>
<pre><code class="lang-py">plt.scatter(X_valid_2D[:, <span class="hljs-number">0</span>], X_valid_2D[:, <span class="hljs-number">1</span>], c=y_valid, s=<span class="hljs-number">10</span>, cmap=<span class="hljs-string">"tab10"</span>)
</code></pre>
<p>图 17-5 展示了结果的散点图（并展示了一些图片）。t-SNE 算法区分除了几类，比较符合图片的类别（每个类的颜色不一样）。</p>
<p><img src="img/fc72779301071ddc44cc9423b21732bc.png" alt=""></img></p>
<p>图 17-5 使用自编码器和 t-SNE 对 Fashion MNIST 做可视化</p>
<p>自编码器的另一个用途是无监督预训练。</p>
<h3 id="使用栈式自编码器做无监督预训练">使用栈式自编码器做无监督预训练</h3>
<p>第 11 章讨论过，如果要处理一个复杂的监督任务，但又缺少标签数据，解决的方法之一，是找一个做相似任务的神经网络，复用它的底层。这么做就可以使用少量训练数据训练出高性能的模型，因为模型不必学习所有低层次特征；模型可以复用之前的特征探测器。</p>
<p>相似的，如果有一个大数据集，但大部分实例是无标签的，可以用全部数据训练一个栈式自编码器，然后使用其底层创建一个神经网络，再用有标签数据来训练。例如，图 17-6 展示了如何使用栈式自编码器来做分类的无监督预训练。当训练分类器时，如果标签数据不足，可以冻住预训练层（底层）。</p>
<p><img src="img/6a262b3ada6d315c0ff9d176785f0e0d.png" alt=""></img></p>
<p>图 17-6 使用自编码器做无监督预训练</p>
<blockquote>
<p>笔记：无标签数据很多，有标签数据数据很少，非常普遍。搭建一个大无便签数据集很便宜（比如，一段小脚本可以从网上下载许多图片），但是给这些图片打标签（比如，将其标签为可爱或不可爱）只有人做才靠谱。打标签又耗时又耗钱，所以人工标注实例有几千就不错了。</p>
</blockquote>
<p>代码实现没有特殊之处：用所有训练数据训练自编码器，然后用编码器层创建新的神经网络（本章有练习题例子）。</p>
<p>接下来，看看关联权重的方法。</p>
<h3 id="关联权重">关联权重</h3>
<p>当自编码器整齐地对称时，就像我们刚刚构建的那样，一种常用方法是将解码器层的权重与编码器层的权重相关联。 这样减半了模型中的权重数量，加快了训练速度，并限制了过度拟合的风险。具体来说，如果自编码器总共具有<code>N</code>个层（不算输入层），并且<code>W[L]</code>表示第<code>L</code>层的连接权重（例如，层 1 是第一隐藏层，则层<code>N / 2</code>是编码层，而层<code>N</code>是输出层），则解码器层权重可以简单地定义为：<code>W[N–L+1] = W[L]^T</code>（其中<code>L = 1, 2, ..., N/2</code>）。</p>
<p>使用 Keras 将层的权重关联起来，先定义一个自定义层：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DenseTranspose</span>(keras.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dense, activation=<span class="hljs-literal">None</span>, **kwargs</span>):
        <span class="hljs-variable language_">self</span>.dense = dense
        <span class="hljs-variable language_">self</span>.activation = keras.activations.get(activation)
        <span class="hljs-built_in">super</span>().__init__(**kwargs)
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self, batch_input_shape</span>):
        <span class="hljs-variable language_">self</span>.biases = <span class="hljs-variable language_">self</span>.add_weight(name=<span class="hljs-string">"bias"</span>, initializer=<span class="hljs-string">"zeros"</span>,
                                      shape=[<span class="hljs-variable language_">self</span>.dense.input_shape[-<span class="hljs-number">1</span>]])
        <span class="hljs-built_in">super</span>().build(batch_input_shape)
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs</span>):
        z = tf.matmul(inputs, <span class="hljs-variable language_">self</span>.dense.weights[<span class="hljs-number">0</span>], transpose_b=<span class="hljs-literal">True</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.activation(z + <span class="hljs-variable language_">self</span>.biases)
</code></pre>
<p>自定义层的作用就像一个常规紧密层，但使用了另一个紧密层的权重，并且做了转置（设置<code>transpose_b=True</code>等同于转置第二个参数，但在<code>matmul()</code>运算中实时做转置更为高效）。但是，要使用自己的偏置向量。然后，创建一个新的栈式自编码器，将解码器的紧密层和编码器的紧密层关联起来：</p>
<pre><code class="lang-py">dense_1 = keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>)
dense_2 = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">"selu"</span>)

tied_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>]),
    dense_1,
    dense_2
])

tied_decoder = keras.models.Sequential([
    DenseTranspose(dense_2, activation=<span class="hljs-string">"selu"</span>),
    DenseTranspose(dense_1, activation=<span class="hljs-string">"sigmoid"</span>),
    keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])
])

tied_ae = keras.models.Sequential([tied_encoder, tied_decoder])
</code></pre>
<p>这个模型的重建误差小于前一个模型，且参数量只有一半。</p>
<h3 id="一次训练一个自编码器">一次训练一个自编码器</h3>
<p>不是一次完成整个栈式自编码器的训练，而是一次训练一个浅自编码器，然后将所有这些自编码器堆叠到一个栈式自编码器（因此名称）中，通常要快得多，如图 17-7 所示。 这个方法如今用的不多了，但偶尔还会撞见谈到“贪婪层级训练”的论文，所以还是看一看。</p>
<p><img src="img/2e97115a3976ddb176e401dcfa95b53c.png" alt=""></img></p>
<p>图 17-7 一次训练一个自编码器</p>
<p>在训练的第一阶段，第一个自编码器学习重构输入。 然后，使用整个训练集训练第一个自编码器，得到一个新的（压缩过的）训练集。然后用这个数据集训练第二个自编码器。这是第二阶段的训练。最后，我们用所有这些自编码器创建一个三明治结构，见图 17-7（即，先把每个自编码器的隐藏层叠起来，再加上输出层）。这样就得到了最终的栈式自编码器（见笔记本）。我们可以用这种方式训练更多的自编码器，搭建非常深的栈式自编码器。</p>
<p>正如前面讨论过的，现在的一大趋势是 Geoffrey Hinton 等人在 2006 年发现的，靠这种贪婪层级方法，可以用无监督方式训练神经网络。他们还使用了受限玻尔兹曼机（RBM，见附录 E）。但在 2007 年，Yoshua Bengio 发现只用自编码器也可以达到不错的效果。在这几年间，自编码器是唯一的有效训练深度网络的方法，知道出现第 11 章介绍过的方法。</p>
<p>自编码器不限于紧密网络：还有卷积自编码器和循环自编码器。</p>
<h2 id="卷积自编码器">卷积自编码器</h2>
<p>如果处理的是图片，则前面介绍的自编码器的效果可能一般（除非图片非常小）。第 14 章介绍过，对于图片任务，卷积神经网络比紧密网络的效果更好。所以如果想用自编码器来处理图片的话（例如，无监督预训练或降维），你需要搭建一个卷积自编码器。编码器是一个包含卷积层和池化层的常规 CNN。通常降低输入的空间维度（即，高和宽），同时增加深度（即，特征映射的数量）。解码器的工作相反（放大图片，压缩深度），要这么做的话，可以转置卷积层（或者，可以将上采样层和卷积层合并）。下面是一个卷积自编码器处理 Fashion MNIST 的例子：</p>
<pre><code class="lang-py">conv_encoder = keras.models.Sequential([
    keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>], input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>]),
    keras.layers.Conv2D(<span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>),
    keras.layers.Conv2D(<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>),
    keras.layers.Conv2D(<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>)
])
conv_decoder = keras.models.Sequential([
    keras.layers.Conv2DTranspose(<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"valid"</span>,
                                 activation=<span class="hljs-string">"selu"</span>,
                                 input_shape=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>]),
    keras.layers.Conv2DTranspose(<span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"same"</span>,
                                 activation=<span class="hljs-string">"selu"</span>),
    keras.layers.Conv2DTranspose(<span class="hljs-number">1</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"same"</span>,
                                 activation=<span class="hljs-string">"sigmoid"</span>),
    keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])
])
conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])
</code></pre>
<h2 id="循环自编码器">循环自编码器</h2>
<p>如果你想用自编码器处理序列，比如对时间序列或文本无监督学习和降维，则循环神经网络要优于紧密网络。搭建循环自编码器很简单：编码器是一个序列到向量的 RNN，而解码器是向量到序列的 RNN：</p>
<pre><code class="lang-py">recurrent_encoder = keras.models.Sequential([
    keras.layers.LSTM(<span class="hljs-number">100</span>, return_sequences=<span class="hljs-literal">True</span>, input_shape=[<span class="hljs-literal">None</span>, <span class="hljs-number">28</span>]),
    keras.layers.LSTM(<span class="hljs-number">30</span>)
])
recurrent_decoder = keras.models.Sequential([
    keras.layers.RepeatVector(<span class="hljs-number">28</span>, input_shape=[<span class="hljs-number">30</span>]),
    keras.layers.LSTM(<span class="hljs-number">100</span>, return_sequences=<span class="hljs-literal">True</span>),
    keras.layers.TimeDistributed(keras.layers.Dense(<span class="hljs-number">28</span>, activation=<span class="hljs-string">"sigmoid"</span>))
])
recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])
</code></pre>
<p>这个循环自编码器可以处理任意长度的序列，每个时间步有 28 个维度。这意味着，可以将 Fashion MNIST 的图片作为几行序列来处理。注意，解码器第一层用的是<code>RepeatVector</code>，以保证在每个时间步将输入向量传给解码器。</p>
<p>我们现在已经看过了多种自编码器（基本的、栈式的、卷积的、循环的），学习了训练的方法（一次性训练或逐层训练）。还学习了两种应用：视觉可视化和无监督学习。</p>
<p>为了让自编码学习特征，我们限制了编码层的大小（使它处于不完整的状态）。还可以使用许多其他的限制方法，可以让编码层和输入层一样大，甚至更大，得到一个过完成的自编码器。下面就是其中一些方法。</p>
<h2 id="降噪自编码">降噪自编码</h2>
<p>另一种强制自编码器学习特征的方法是为其输入添加噪声，对其进行训练以恢复原始的无噪声输入。 自 20 世纪 80 年代以来，使用自编码器消除噪音的想法已经出现（例如，在 Yann LeCun 的 1987 年硕士论文中提到过）。 在 2008 年的一篇论文中，帕斯卡尔文森特等人。 表明自编码器也可用于特征提取。 在 2010 年的一篇炉温中， Vincent 等人引入了栈式降噪自编码器。</p>
<p>噪声可以是添加到输入的纯高斯噪声，或者可以随机关闭输入，就像丢弃（在第 11 章介绍）。 图 17-8 显示了这两种方法。</p>
<p><img src="img/34a85484de696688796d6e35f8e7a0a3.png" alt=""></img></p>
<p>图 17-8 高斯噪音（左）和丢弃（右）的降噪自编码器</p>
<p>实现很简单：常规的栈式自编码器中有一个应用于输入的<code>Dropout</code>层（或使用<code>GaussianNoise</code>层）。<code>Dropout</code>层只在训练中起作用（<code>GaussianNoise</code>层也只在训练中起作用）：</p>
<pre><code class="lang-py">dropout_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>]),
    keras.layers.Dropout(<span class="hljs-number">0.5</span>),
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">"selu"</span>)
])
dropout_decoder = keras.models.Sequential([
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>, input_shape=[<span class="hljs-number">30</span>]),
    keras.layers.Dense(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, activation=<span class="hljs-string">"sigmoid"</span>),
    keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])
])
dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])
</code></pre>
<p>图 17-9 展示了一些带有造影的图片（有一半像素被丢弃），重建图片是用基于丢弃的自编码器实现的。注意自编码器是如何猜测图片中不存在的细节的，比如四张图片的领口。</p>
<p><img src="img/d726c67a612e5268f2123baf0d028222.png" alt=""></img></p>
<p>图 17-9 噪音图片（上）和重建图片（下）</p>
<h2 id="稀疏自编码器">稀疏自编码器</h2>
<p>通常良好特征提取的另一种约束是稀疏性：通过向损失函数添加适当的项，让自编码器减少编码层中活动神经元的数量。 例如，可以让编码层中平均只有 5% 的活跃神经元。 这迫使自编码器将每个输入表示为少量激活的组合。 因此，编码层中的每个神经元通常都会代表一个有用的特征（如果每个月只能说几个字，你会说的特别精炼）。</p>
<p>使用 sigmoid 激活函数可以实现这个目的。添加一个编码层（比如，有 300 个神经元），给编码层的激活函数添加<code>ℓ1</code>正则（解码器就是一个常规解码器）：</p>
<pre><code class="lang-py">sparse_l1_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>]),
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.Dense(<span class="hljs-number">300</span>, activation=<span class="hljs-string">"sigmoid"</span>),
    keras.layers.ActivityRegularization(l1=<span class="hljs-number">1e-3</span>)
])
sparse_l1_decoder = keras.models.Sequential([
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>, input_shape=[<span class="hljs-number">300</span>]),
    keras.layers.Dense(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, activation=<span class="hljs-string">"sigmoid"</span>),
    keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])
])
sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])
</code></pre>
<p><code>ActivityRegularization</code>只是返回输入，但副作用是新增了训练损失，大小等于输入的绝对值之和（这个层只在训练中起作用）。等价的，可以移出<code>ActivityRegularization</code>，并在前一层设置<code>activity_regularizer=keras.regularizers.l1(1e-3)</code>。这项惩罚可以让神经网络产生接近 0 的编码，如果没有正确重建输入，还是会有损失，仍然会产生一些非 0 值。不使用<code>ℓ2</code>，而使用<code>ℓ1</code>，可以让神经网络保存最重要的编码，同时消除输入图片不需要的编码（而不是压缩所有编码）。</p>
<p>另一种结果更好的方法是在每次训练迭代中测量编码层的实际稀疏度，当偏移目标值，就惩罚模型。 我们通过计算整个训练批次中编码层中每个神经元的平均激活来实现。 批量大小不能太小，否则平均数不准确。</p>
<p>一旦我们对每个神经元进行平均激活，我们希望通过向损失函数添加稀疏损失来惩罚太活跃的神经元，或不够活跃的神经元。 例如，如果我们测量一个神经元的平均激活值为 0.3，但目标稀疏度为 0.1，那么它必须受到惩罚才能激活更少。 一种方法可以简单地将平方误差<code>(0.3-0.1)^2</code>添加到损失函数中，但实际上更好的方法是使用 Kullback-Leibler 散度（在第 4 章中简要讨论），它具有比均方误差更强的梯度，如图 17-10 所示。</p>
<p><img src="img/3dfe7472d69611d43ccd638d7eb71169.png" alt=""></img></p>
<p>图 17-10 稀疏损失</p>
<p>给定两个离散的概率分布<code>P</code>和<code>Q</code>，这些分布之间的 KL 散度，记为<code>D[KL](P || Q)</code>，可以使用公式 17-1 计算。</p>
<p><img src="img/84f8b1f35d7af4b1e2321b25be6f00d9.png" alt=""></img></p>
<p>公式 17-1 Kullback–Leibler 散度</p>
<p>在我们的例子中，我们想要测量编码层中的神经元将激活的目标概率<code>p</code>与实际概率<code>q</code>（即，训练批次上的平均激活）之间的差异。 所以 KL 散度简化为公式 17-2。</p>
<p><img src="img/96a7a903ca020a64ae2a2e839ff98ac1.png" alt=""></img></p>
<p>公式 17-2 目标稀疏度<code>p</code>和实际稀疏度<code>q</code>之间的 KL 散度</p>
<p>一旦我们已经计算了编码层中每个神经元的稀疏损失，就相加这些损失，并将结果添加到损失函数中。 为了控制稀疏损失和重构损失的相对重要性，我们可以用稀疏权重超参数乘以稀疏损失。 如果这个权重太高，模型会紧贴目标稀疏度，但它可能无法正确重建输入，导致模型无用。 相反，如果它太低，模型将大多忽略稀疏目标，它不会学习任何有趣的功能。</p>
<p>现在就可以实现基于 KL 散度的稀疏自编码器了。首先，创建一个自定义正则器来实现 KL 散度正则：</p>
<pre><code class="lang-py">K = keras.backend
kl_divergence = keras.losses.kullback_leibler_divergence

<span class="hljs-keyword">class</span> <span class="hljs-title class_">KLDivergenceRegularizer</span>(keras.regularizers.Regularizer):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, weight, target=<span class="hljs-number">0.1</span></span>):
        <span class="hljs-variable language_">self</span>.weight = weight
        <span class="hljs-variable language_">self</span>.target = target
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inputs</span>):
        mean_activities = K.mean(inputs, axis=<span class="hljs-number">0</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.weight * (
            kl_divergence(<span class="hljs-variable language_">self</span>.target, mean_activities) +
            kl_divergence(<span class="hljs-number">1</span>\. - <span class="hljs-variable language_">self</span>.target, <span class="hljs-number">1</span>\. - mean_activities))
</code></pre>
<p>使用<code>KLDivergenceRegularizer</code>作为编码层的激活函数，创建稀疏自编码器：</p>
<pre><code class="lang-py">kld_reg = KLDivergenceRegularizer(weight=<span class="hljs-number">0.05</span>, target=<span class="hljs-number">0.1</span>)
sparse_kl_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>]),
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.Dense(<span class="hljs-number">300</span>, activation=<span class="hljs-string">"sigmoid"</span>, activity_regularizer=kld_reg)
])
sparse_kl_decoder = keras.models.Sequential([
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>, input_shape=[<span class="hljs-number">300</span>]),
    keras.layers.Dense(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, activation=<span class="hljs-string">"sigmoid"</span>),
    keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])
])
sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])
</code></pre>
<p>在 Fashion MNIST 上训练好稀疏自编码器之后，编码层中的神经元的激活大部分接近 0（70% 的激活小于 0.1），所有神经元的平均值在 0.1 附近（90% 的平均激活在 0.1 和 0.2 之间）见图 17-11。</p>
<p><img src="img/2de4723c6249ec91b3cb15bc31d64b7f.png" alt=""></img></p>
<p>图 17-11 编码层的所有激活的分布（左）和每个神经元平均激活的分布（右）</p>
<h2 id="变分自编码器（vae）">变分自编码器（VAE）</h2>
<p>Diederik Kingma 和 Max Welling 于 2013 年推出了另一类重要的自编码器，并迅速成为最受欢迎的自编码器类型之一：变分自编码器。</p>
<p>它与我们迄今为止讨论的所有自编码器非常不同，特别是：</p>
<ul>
<li><p>它们是概率自编码器，意味着即使在训练之后，它们的输出部分也是偶然确定的（相对于仅在训练过程中使用随机性的自编码器的去噪）。</p>
</li>
<li><p>最重要的是，它们是生成自编码器，这意味着它们可以生成看起来像从训练集中采样的新实例。</p>
</li>
</ul>
<p>这两个属性使它们与 RBM 非常相似（见附录 E），但它们更容易训练，并且取样过程更快（在 RBM 之前，您需要等待网络稳定在“热平衡”之后才能进行取样一个新的实例）。正如其名字，变分自编码器要做变分贝叶斯推断（第 9 章介绍过），这是估计变微分推断的一种有效方式。</p>
<p>我们来看看他们是如何工作的。 图 17-12（左）显示了一个变分自编码器。 当然，您可以认识到所有自编码器的基本结构，编码器后跟解码器（在本例中，它们都有两个隐藏层），但有一个转折点：不是直接为给定的输入生成编码 ，编码器产生平均编码<code>μ</code>和标准差<code>σ</code>。 然后从平均值<code>μ</code>和标准差<code>σ</code>的高斯分布随机采样实际编码。 之后，解码器正常解码采样的编码。 该图的右侧部分显示了一个训练实例通过此自编码器。 首先，编码器产生<code>μ</code>和<code>σ</code>，随后对编码进行随机采样（注意它不是完全位于<code>μ</code>处），最后对编码进行解码，最终的输出与训练实例类似。</p>
<p><img src="img/7c9060e3ee9cbc624bfadb3ac589452a.png" alt=""></img></p>
<p>图 17-12 变分自编码器（左）和一个执行中的实例（右）</p>
<p>从图中可以看出，尽管输入可能具有非常复杂的分布，但变分自编码器倾向于产生编码，看起来好像它们是从简单的高斯分布采样的：在训练期间，损失函数（将在下面讨论）推动 编码在编码空间（也称为潜在空间）内逐渐迁移以占据看起来像高斯点集成的云的大致（超）球形区域。 一个重要的结果是，在训练了一个变分自编码器之后，你可以很容易地生成一个新的实例：只需从高斯分布中抽取一个随机编码，对它进行解码就可以了！</p>
<p>再来看看损失函数。 它由两部分组成。 首先是通常的重建损失，推动自编码器重现其输入（我们可以使用交叉熵来解决这个问题，如前所述）。 第二种是潜在的损失，推动自编码器使编码看起来像是从简单的高斯分布中采样，为此我们使用目标分布（高斯分布）与编码实际分布之间的 KL 散度。 数学比以前复杂一点，特别是因为高斯噪声，它限制了可以传输到编码层的信息量（从而推动自编码器学习有用的特征）。 幸好，这些方程经过简化，可以用公式 17-3 计算潜在损失：</p>
<p><img src="img/9273964ee58f08a1c76543fe9af21c40.png" alt=""></img></p>
<p>公式 17-3 变分自编码器的潜在损失</p>
<p>在这个公式中，<code>L</code>是潜在损失，<code>n</code>是编码维度，<code>μ[i]</code>和<code>σ[i]</code>是编码的第<code>i</code>个成分的平均值和标准差。向量<code>μ</code>和<code>σ</code>是编码器的输出，见图 17-12 的左边。</p>
<p>一种常见的变体是训练编码器输出<code>γ= log(σ^2)</code>而不是<code>σ</code>。 可以用公式 17-4 计算潜在损失。这个方法的计算更稳定，且可以加速训练。</p>
<p><img src="img/8d10a31df92b1efadc78aa7882e885c0.png" alt=""></img></p>
<p>公式 17-4 变分自编码器的潜在损失，使用<code>γ = log(σ^2)</code></p>
<p>给 Fashion MNIST 创建一个自编码器（见图 17-12，使用<code>γ</code>变体）。首先，需要一个自定义层从编码采样，给定<code>μ</code>和<code>γ</code>：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sampling</span>(keras.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs</span>):
        mean, log_var = inputs
        <span class="hljs-keyword">return</span> K.random_normal(tf.shape(log_var)) * K.exp(log_var / <span class="hljs-number">2</span>) + mean
</code></pre>
<p>这个<code>Sampling</code>层接收两个输入：<code>mean (μ)</code> 和 <code>log_var (γ)</code>。使用函数<code>K.random_normal()</code>根据正态分布随机采样向量（形状为<code>γ</code>）平均值为 0 标准差为 1。然后乘以<code>exp(γ / 2)</code>（这个值等于<code>σ</code>），最后加上μ并返回结果。这样就能从平均值为 0 标准差为 1 的正态分布采样编码向量。</p>
<p>然后，创建编码器，因为模型不是完全顺序的，所以要使用函数式 API：</p>
<pre><code class="lang-py">codings_size = <span class="hljs-number">10</span>

inputs = keras.layers.Input(shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])
z = keras.layers.Flatten()(inputs)
z = keras.layers.Dense(<span class="hljs-number">150</span>, activation=<span class="hljs-string">"selu"</span>)(z)
z = keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>)(z)
codings_mean = keras.layers.Dense(codings_size)(z)  <span class="hljs-comment"># μ</span>
codings_log_var = keras.layers.Dense(codings_size)(z)  <span class="hljs-comment"># γ</span>
codings = Sampling()([codings_mean, codings_log_var])
variational_encoder = keras.Model(
    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])
</code></pre>
<p>注意，输出<code>codings_mean</code> (μ)和<code>codings_log_var</code> (γ)的<code>Dense</code>层，有同样的输入（即，第二个紧密层的输出）。然后将<code>codings_mean</code>和<code>codings_log_var</code>传给<code>Sampling</code>层。最后，<code>variational_encoder</code>模型有三个输出，可以用来检查<code>codings_mean</code>和<code>codings_log_var</code>的值。真正使用的是最后一个（<code>codings</code>）。下面创建解码器：</p>
<pre><code class="lang-py">decoder_inputs = keras.layers.Input(shape=[codings_size])
x = keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>)(decoder_inputs)
x = keras.layers.Dense(<span class="hljs-number">150</span>, activation=<span class="hljs-string">"selu"</span>)(x)
x = keras.layers.Dense(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, activation=<span class="hljs-string">"sigmoid"</span>)(x)
outputs = keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])(x)
variational_decoder = keras.Model(inputs=[decoder_inputs], outputs=[outputs])
</code></pre>
<p>对于解码器，因为是简单栈式结构，可以不使用函数式 API，而使用顺序 API。最后，创建变分自编码器：</p>
<pre><code class="lang-py">_, _, codings = variational_encoder(inputs)
reconstructions = variational_decoder(codings)
variational_ae = keras.Model(inputs=[inputs], outputs=[reconstructions])
</code></pre>
<p>注意，我们忽略了编码器的前两个输出。最后，必须将潜在损失和重建损失加起来：</p>
<pre><code class="lang-py">latent_loss = -<span class="hljs-number">0.5</span> * K.<span class="hljs-built_in">sum</span>(
    <span class="hljs-number">1</span> + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),
    axis=-<span class="hljs-number">1</span>)
variational_ae.add_loss(K.mean(latent_loss) / <span class="hljs-number">784.</span>)
variational_ae.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">"binary_crossentropy"</span>, optimizer=<span class="hljs-string">"rmsprop"</span>)
</code></pre>
<p>我们首先用公式 17-4 计算批次中每个实例的潜在损失。然后计算所有实例的平均损失，然后除以，使其量纲与重建损失一致。实际上，变分自编码器的重建损失是像素重建误差的和，但当 Keras 计算<code>"binary_crossentropy"</code>损失时，它计算的是 784 个像素的平均值，而不是和。因此，重建损失比真正要的值小 784 倍。我们可以定义一个自定义损失来计算误差和，但除以 784 更简单。</p>
<p>注意，这里使用了<code>RMSprop</code>优化器。最后，我们可以训练自编码器。</p>
<pre><code class="lang-py">history = variational_ae.fit(X_train, X_train, epochs=<span class="hljs-number">50</span>, batch_size=<span class="hljs-number">128</span>,
                             validation_data=[X_valid, X_valid])
</code></pre>
<h3 id="生成-fashion-mnist-图片">生成 Fashion MNIST 图片</h3>
<p>接下来用上面的变分自编码器生成图片。我们要做的只是从高斯分布随机采样编码，然后做解码：</p>
<pre><code class="lang-py">codings = tf.random.normal(shape=[<span class="hljs-number">12</span>, codings_size])
images = variational_decoder(codings).numpy()
</code></pre>
<p>图 17-13 展示了 12 张生成的图片。</p>
<p><img src="img/1c696a3cda652b27fb2296c17a087922.png" alt=""></img></p>
<p>图 17-13 用变分自编码器生成的 Fashion MNIST 图片</p>
<p>大多数生成的图片很逼真，就是有些模糊。其它的效果一般，这是因为自编码器只学习了几分钟。经过微调和更长时间的训练，效果就能编号。</p>
<p>变分自编码器也可以做语义插值：不是对两张图片做像素级插值（结果就像是两张图重叠），而是在编码级插值。先用编码层运行两张图片，然后对两个编码层插值，然后解码插值编码，得到结果图片。结果就像一个常规的 Fashion MINIST 图片，但还是介于原始图之间。在接下来的代码中，将 12 个生成出来的编码，排列成<code>3 × 4</code>的网格，然后用 TensorFlow 的<code>tf.image.resize()</code>函数，将其缩放为<code>5 × 7</code>。默认条件下，<code>resize()</code>函数会做双线性插值，所以每两个行或列都会包含插值编码。然后用解码器生成所有图片：</p>
<pre><code class="lang-py">codings_grid = tf.reshape(codings, [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, codings_size])
larger_grid = tf.image.resize(codings_grid, size=[<span class="hljs-number">5</span>, <span class="hljs-number">7</span>])
interpolated_codings = tf.reshape(larger_grid, [-<span class="hljs-number">1</span>, codings_size])
images = variational_decoder(interpolated_codings).numpy()
</code></pre>
<p>图 17-14 展示了结果。画框的是原始图，其余是根据附近图片做出的语义插值图。注意，第 4 行第 5 列的鞋，是上下两张图的完美融合。</p>
<p><img src="img/f4ffcbb836de51fac9c67bb2c4970800.png" alt=""></img></p>
<p>图 17-14 语义插值</p>
<p>变分自编码器流行几年之后，就被 GAN 超越了，后者可以生成更为真实的图片。</p>
<h2 id="对抗生成网络（gan）">对抗生成网络（GAN）</h2>
<p>对抗生成网络是 Ian Goodfellow 在 2014 年的一篇<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fhoml.info%2Fgan" target="_blank">论文</a>中提出的，尽管一开始就引起了众人的兴趣，但用了几年时间才客服了训练 GAN 的一些难点。和其它伟大的想法一样，GAN 的本质很简单：让神经网络互相竞争，让其在竞争中进步。见图 17-15，GAN 包括两个神经网络：</p>
<ul>
<li><p>生成器
使用随机分布作为输入（通常为高斯分布），并输出一些数据，比如图片。可以将随机输入作为生成文件的潜在表征（即，编码）。生成器的作用和变分自编码器中的解码器差不多，可以用同样的方式生成图片（只要输入一些高斯噪音，就能输出全新的图片）。但是，生成器的训练过程很不一样。</p>
</li>
<li><p>判别器
从训练集取出一张图片，判断图片是真是假。</p>
</li>
</ul>
<p><img src="img/439b37e1638f28229762d9a05a371c77.png" alt=""></img></p>
<p>图 17-15 一个对抗生成网络</p>
<p>在训练中，生成器和判别器的目标正好相反：判别器判断图片的真假，生成器尽力生成看起来像真图的图片。因为 GAN 由这两个目的不同的网络组成，所以不能像常规网络那样训练。每次训练迭代分成两个阶段：</p>
<ul>
<li><p>第一个阶段，训练判别器。从训练集取样一批真实图片，数量与假图片相同。假图片的标签设为 0，真图片的标签设为 1，判别器用这个有标签的批次训练一步，使用二元交叉熵损失。反向传播在这一阶段只优化判别器的权重。</p>
</li>
<li><p>第二个阶段，训练生成器。首先用生成器产生另一个批次的假图片，再用判别器来判断图片是真是假。这一次不添加真图片，但所有标签都设为 1（真）：换句话说，我们想让生成器产生可以让判别器信以为真的图片。判别器的权重在这一步是冷冻的，所以反向传播只影响生成器。</p>
</li>
</ul>
<blockquote>
<p>笔记：生成器看不到真图，但却逐渐生成出逼真的不骗。它只是使用了经过判别器返回的梯度。幸好，随着判别器的优化，这些二手梯度中包含的关于真图的信息也越来越多，所以生成器才能进步。</p>
</blockquote>
<p>接下来为 Fashion MNIST 创建一个简单的 GAN 模型。</p>
<p>首先，创建生成器和判别器。生成器很像自编码器的解码器，判别器就是一个常规的二元分类器（图片作为输入，输出是包含一个神经元的紧密层，使用 sigmoid 激活函数）。对于每次训练迭代中的第二阶段，需要完整的 GAN 模型：</p>
<pre><code class="lang-py">codings_size = <span class="hljs-number">30</span>

generator = keras.models.Sequential([
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>, input_shape=[codings_size]),
    keras.layers.Dense(<span class="hljs-number">150</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.Dense(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, activation=<span class="hljs-string">"sigmoid"</span>),
    keras.layers.Reshape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>])
])
discriminator = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>]),
    keras.layers.Dense(<span class="hljs-number">150</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">"selu"</span>),
    keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">"sigmoid"</span>)
])
gan = keras.models.Sequential([generator, discriminator])
</code></pre>
<p>然后，我们需要编译这些模型。因为判别器是一个二元分类器，我们可以使用二元交叉熵损失。生成器只能通过 GAN 训练，所以不需要编译生成器。<code>gan</code>模型也是一个二元分类器，所以可以使用二元交叉熵损失。重要的，不能在第二个阶段训练判别器，所以编译模型之前，使其不可训练：</p>
<pre><code class="lang-py">discriminator.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">"binary_crossentropy"</span>, optimizer=<span class="hljs-string">"rmsprop"</span>)
discriminator.trainable = <span class="hljs-literal">False</span>
gan.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">"binary_crossentropy"</span>, optimizer=<span class="hljs-string">"rmsprop"</span>)
</code></pre>
<blockquote>
<p>笔记：Keras 只有在编译模型时才会考虑<code>trainable</code>属性，所以运行这段代码后，如果调用<code>fit()</code>方法或<code>train_on_batch()</code>方法，<code>discriminator</code>就是可训练的了。但在<code>gan</code>模型上调用这些方法，判别器是不可训练的。</p>
</blockquote>
<p>因为训练循环是非常规的，我们不能使用常规的<code>fit()</code>方法。但我们可以写一个自定义的训练循环。要这么做，需要先创建一个<code>Dataset</code>迭代这些图片：</p>
<pre><code class="lang-py">batch_size = <span class="hljs-number">32</span>
dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(<span class="hljs-number">1000</span>)
dataset = dataset.batch(batch_size, drop_remainder=<span class="hljs-literal">True</span>).prefetch(<span class="hljs-number">1</span>)
</code></pre>
<p>现在就可以来写训练循环了。用<code>train_gan()</code>函数来包装：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_gan</span>(<span class="hljs-params">gan, dataset, batch_size, codings_size, n_epochs=<span class="hljs-number">50</span></span>):
    generator, discriminator = gan.layers
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):
        <span class="hljs-keyword">for</span> X_batch <span class="hljs-keyword">in</span> dataset:
            <span class="hljs-comment"># phase 1 - training the discriminator</span>
            noise = tf.random.normal(shape=[batch_size, codings_size])
            generated_images = generator(noise)
            X_fake_and_real = tf.concat([generated_images, X_batch], axis=<span class="hljs-number">0</span>)
            y1 = tf.constant([[<span class="hljs-number">0.</span>]] * batch_size + [[<span class="hljs-number">1.</span>]] * batch_size)
            discriminator.trainable = <span class="hljs-literal">True</span>
            discriminator.train_on_batch(X_fake_and_real, y1)
            <span class="hljs-comment"># phase 2 - training the generator</span>
            noise = tf.random.normal(shape=[batch_size, codings_size])
            y2 = tf.constant([[<span class="hljs-number">1.</span>]] * batch_size)
            discriminator.trainable = <span class="hljs-literal">False</span>
            gan.train_on_batch(noise, y2)

train_gan(gan, dataset, batch_size, codings_size)
</code></pre>
<p>和前面讨论的一样，每次迭代都有两个阶段：</p>
<ul>
<li><p>在第一阶段，向生成器输入高斯噪音来生成假图片，然后再补充同等数量的真图片。假图片的目标<code>y1</code>设为 0，真图片的目标<code>y1</code>设为 1。然后用这个批次训练判别器。注意，将判别器的<code>trainable</code>属性设为<code>True</code>：这是为了避免 Keras 检查到现在是<code>False</code>而在训练时为<code>True</code>，显示警告。</p>
</li>
<li><p>在第二阶段，向 GAN 输入一些高斯噪音。它的生成器会开始假图片，然后判别器会判断其真假。我们希望判别器判断图片是真的，所以<code>y2</code>设为 1。注意，为了避免警告，将<code>trainable</code>属性设为<code>False</code>。</p>
</li>
</ul>
<p>这样就好了！如果展示生成出来的图片（见图 17-16），可以看到在第一个周期的后期，图片看起来已经接近 Fashion MNIST 的图片了。</p>
<p><img src="img/49a9afbd5ba01e0eb2b98f7000c9f94e.png" alt=""></img></p>
<p>图 17-16 GAN 训练一个周期后，生成的图片</p>
<p>不过，再怎么训练，图片的质量并没有提升，还发现在有的周期 GAN 完全忘了学到了什么。为什么会这样？貌似训练 GAN 很有挑战。接下来看看原因。</p>
<h3 id="训练-gan-的难点">训练 GAN 的难点</h3>
<p>在训练中，生成器和判别器不断试图超越对方，这是一个零和博弈。随着训练的进行，可能会达成博弈学家称为纳什均衡的状态：每个选手都不改变策略，并认为对方也不会改变策略。例如，当所有司机都靠左行驶时，就达到了纳什均衡：没有司机会选择换边。当然，也有第二种可能：每个人都靠右行驶。不同的初始状态和动力学会导致不同的均衡。在这个例子中，达到均衡时，只有一种最优策略，但纳什均衡包括多种竞争策略（比如，捕食者追逐猎物，猎物试图逃跑，两者都要改变策略）。</p>
<p>如何将博弈论应用到 GAN 上呢？论文作者证明，GAN 只能达到一种均衡状态：生成器产生完美的真实图片，同时让判别器来判断（50% 为真，50% 为假）。这是件好事：看起来只要训练 GAN 足够久，就会达到均衡，获得完美的生成器。不过，并没有这么简单：没有人能保证一定能达到均衡。</p>
<p>最大的困难是模式坍塌：生成器的输出逐渐变得不那么丰富。为什么会这样？假设生成器产生的鞋子图片比其它类的图片更让人信服，假鞋子图片就会更多的欺骗判别器，就会导致生成更多的鞋子图片。逐渐的，生成器会忘掉如何生成其它类的图片。同时，判别器唯一能看到的就是鞋子图片，所以判别器也会忘掉如何判断其它类的图片。最终，当判别器想要区分假鞋和真鞋时，生成器会被迫生成其它类。生成器可能变成善于衬衫，而忘了鞋子，判别器也会发生同样的转变。GAN 会逐渐在一些类上循环，从而对哪一类都不擅长。</p>
<p>另外，因为生成器和判别器不断试探对方，它们的参数可能不断摇摆。训练可能一开始正常，但因为不稳定性，会突然发散。又因为多种因素可能会影响动力学，GAN 会对超参数特别敏感：微调超参数会特别花费时间。</p>
<p>这些问题自从 2014 年就一直困扰着人们：人们发表了许多论文，一些论文提出新的损失函数、或稳定化训练的手段、或避免模式坍塌。例如，经验接力：将生成器在每个迭代产生的图片存储在接力缓存中（逐次丢弃旧的生成图），使用真实图片和从缓存中取出的图片训练判别器。这样可以降低判别器对生成器的最后一个输出过拟合的几率。另外一个方法是小批次判别：测量批次中图片的相似度，然后将数据传给判别器，判别器就可以删掉缺乏散度的假图片。这可以鼓励生成器产生更多类的图片，避免模式坍塌。</p>
<p>总而言之，这是一个非常活跃的研究领域，GAN 的动力学仍然没有彻底搞清。好消息是人们已经取得了一定成果，效果不俗。接下来看看一些成功的架构，从深度卷积 GAN 开始，这是几年前的前沿成果。然后再看两个新近的（更复杂的）架构。</p>
<h3 id="深度卷积-gan">深度卷积 GAN</h3>
<p>2014 年的原始 GAN 论文是用卷积层实验的，但只用来生成小图片。不久之后，许多人使用深度卷积网络为大图片创建 GAN。过程艰难，因为训练不稳定，但最终 Alec Radford 等人试验了许多不同的架构和超参数，在 2015 年取得了成功。他们将最终架构称为深度卷积 GAN（DCGAN）。他们提出的搭建稳定卷积 GAN 的建议如下：</p>
<ul>
<li><p>（判别器中）用卷积步长（strided convolutions）、（生成器中）用转置卷积，替换池化层。</p>
</li>
<li><p>生成器和判别器都使用批归一化，除了生成器的输出层和判别器的输入层。</p>
</li>
<li><p>去除深层架构中的全连接隐藏层。</p>
</li>
<li><p>生成器的输出层使用 tanh 激活，其它层使用 ReLU 激活。</p>
</li>
<li><p>判别器的所有层使用 leaky ReLU 激活。</p>
</li>
</ul>
<p>这些建议在许多任务中有效，但存在例外，所以你还是需要尝试不同的超参数（事实上，改变随机种子，再训练模型，可能就成功了）。例如，下面是一个小型的 DCGAN，在 Fashion MNIST 上效果不错：</p>
<pre><code class="lang-py">codings_size = <span class="hljs-number">100</span>

generator = keras.models.Sequential([
    keras.layers.Dense(<span class="hljs-number">7</span> * <span class="hljs-number">7</span> * <span class="hljs-number">128</span>, input_shape=[codings_size]),
    keras.layers.Reshape([<span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">128</span>]),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2DTranspose(<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"same"</span>,
                                 activation=<span class="hljs-string">"selu"</span>),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2DTranspose(<span class="hljs-number">1</span>, kernel_size=<span class="hljs-number">5</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"same"</span>,
                                 activation=<span class="hljs-string">"tanh"</span>)
])
discriminator = keras.models.Sequential([
    keras.layers.Conv2D(<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"same"</span>,
                        activation=keras.layers.LeakyReLU(<span class="hljs-number">0.2</span>),
                        input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>]),
    keras.layers.Dropout(<span class="hljs-number">0.4</span>),
    keras.layers.Conv2D(<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">5</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"same"</span>,
                        activation=keras.layers.LeakyReLU(<span class="hljs-number">0.2</span>)),
    keras.layers.Dropout(<span class="hljs-number">0.4</span>),
    keras.layers.Flatten(),
    keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">"sigmoid"</span>)
])
gan = keras.models.Sequential([generator, discriminator])
</code></pre>
<p>生成器的编码大小为 100，将其投影到 6272 个维度上（<code>7 * 7 * 128</code>），将结果变形为<code>7 × 7 × 128</code>的张量。这个张量经过批归一化，然后输入给步长为 2 的转置卷积层，从<code>7 × 7</code>上采样为<code>14 × 14</code>，深度从 128 降到 64。结果再做一次批归一化，传给另一个步长为 2 的转置卷积层，从<code>7 × 7</code>上采样为<code>14 × 14</code>，深度从 64 降到 1。这个层使用 tanh 激活函数，输出范围是-1 到 1。因为这个原因，在训练 GAN 之前，需要收缩训练集到相同的范围。还需要变形，加上通道维度：</p>
<pre><code class="lang-py">X_train = X_train.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>) * <span class="hljs-number">2</span>\. - <span class="hljs-number">1</span>\. <span class="hljs-comment"># 变形和收缩</span>
</code></pre>
<p>判别器看起来很像英语二元分类的常规 CNN，除了使用的不是最大池化层降采样图片，而是使用卷积步长。另外，使用的激活函数是 leaky ReLU。</p>
<p>总之，我们遵守了 DCGAN 的建议，除了将判别器中的<code>BatchNormalization</code>替换成了<code>Dropout</code>层（否则训练会变得不稳定），生成器的 ReLU 替换为 SELU。你可以随意调整这个架构：可以看到对超参数（特别是学习率）的敏感度。</p>
<p>最后，要创建数据集，然后编译训练模型，使用和之前一样的代码。经过 50 个周期的训练，生成器的图片见图 17-17。还是不怎么完美，但一些图片已经很逼真了。</p>
<p><img src="img/e21ae4dd116512c19b7838f18c210a5f.png" alt=""></img></p>
<p>图 17-17 DCGAN 经过 50 个周期的训练，生成的图片</p>
<p>如果扩大这个架构，然后用更大的面部数据集训练，可以得到相当逼真的图片。事实上，DCGAN 可以学习到许多有意义的潜在表征，见图 17-18：从生成的诸多图片中手动选取了九张（左上），包括三张戴眼镜的男性，三张不戴眼镜的男性，和三张不戴眼镜的女性。对于每一类，对其编码做平均，用平均的结果再生成一张图片（放在下方）。总之，下方的图片是上方图片的平均。但不是简单的像素平均，而是潜在空间的平均，所以看起来仍是正常的人脸。如果用戴眼镜的男性，减去不戴眼镜的男性，加上不戴眼镜的女性，使用平均编码，就得到了右边<code>3 × 3</code>网格的正中的图片，一个戴眼镜的女性！其它八张是添加了一些噪声的结果，用于解释 DCGAN 的语义插值能力。可以用人脸做加减法就像科幻小说一样！</p>
<p><img src="img/cbb644cd149a74ccf3e2b47fe82e5946.png" alt=""></img></p>
<p>图 17-18 面部的向量运算（来自 DCGAN 论文的图 7）</p>
<blockquote>
<p>提示：如果将图片的类作为另一个输入，输入给生成器和判别器，它们都能学到每个类的样子，你就可以控制生成器产生图片的类。这被称为条件 GAN（CGAN）。</p>
</blockquote>
<p>但是，DCGAN 并不完美。比如，当你使用 DCGAN 生成非常大的图片时，通常是局部逼真，但整体不协调（比如 T 恤的一个袖子比另一个长很多）。如何处理这种问题呢？</p>
<h3 id="gan-的渐进式变大">GAN 的渐进式变大</h3>
<p>Nvidia 研究员 Tero Karras 等人在 2018 年发表了一篇<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fhoml.info%2Fprogan" target="_blank">论文</a>，提出了一个重要方法：他们建议在训练时，先从生成小图片开始，然后逐步给生成器和判别器添加卷积层，生成越来越大的图片（<code>4 × 4, 8 × 8, 16 × 16, …, 512 × 512, 1,024 × 1,024</code>）。这个方法和栈式自编码器的贪婪层级训练很像。余下的层添加到生成器的末端和判别器的前端，之前训练好的层仍然可训练。</p>
<p>例如，当生成器的输出从<code>4 × 4</code>变为<code>4 × 4</code>时（见图 17-19），在现有的卷积层上加上一个上采样层（使用近邻过滤），使其输出<code>4 × 4</code>的特征映射。再接着传给一个新的卷积层（使用<code>same</code>填充，步长为 1，输出为<code>8 × 8</code>）。接着是一个新的输出卷积层：这是一个常规卷积层，核大小为 1，将输出投影到定好的颜色通道上（比如 3）。为了避免破坏第一个训练好的卷积层的权重，最后的输出是原始输出层（现在的输出是<code>4 × 4</code>的特征映射）的权重之和。新输出的权重是α，原始输出的权重是<code>1-α</code>，<code>α</code>逐渐从 0 变为 1。换句话说，新的卷积层（图 17-19 中的虚线）是淡入的，而原始输出层淡出。向判别器（跟着平均池化层做降采样）添加新卷积层时，也是用相似的淡入淡出方法。</p>
<p><img src="img/da86f27b01eca3491106da5569f29e04.png" alt=""></img></p>
<p>图 17-19 GAN 的渐进式变大：GAN 生成器输出<code>4 × 4</code>的彩色图片（左）；将其扩展为<code>4 × 4</code>的图片（右）</p>
<p>这篇文章还提出了一些其它的方法，用于提高输出的散度（避免模式坍塌），使训练更稳定：</p>
<ul>
<li><p>小批次标准差层</p>
<p>添加在判别器的靠近末端的位置。对于输入的每个位置，计算批次（<code>S = tf.math.reduce_std(inputs, axis=[0, -1])</code>）中，所有通道所有实例的标准差。接着，这些标准差对所有点做平均，得到一个单值（<code>v = tf.reduce_​mean(S)</code>）。最后，给批次中的每个实例添加一个额外的特征映射，填入计算得到的单值（<code>tf.concat([inputs, tf.fill([batch_size, height, width, 1], v)], axis=-1)</code>）。这样又什么用呢？如果生成器产生的图片没有什么偏差，则判别器的特征映射的标准差会特别小。有了这个层，判别器就可以做出判断。可以让生成器产生高散度的输出，降低模式坍塌的风险。</p>
</li>
<li><p>相等的学习率</p>
<p>使用一个简单的高斯分布（平均值为 0，标准差为 1）初始化权重，而不使用 He 初始化。但是，权重在运行时（即，每次执行层）会变小：会除以<code>√(2/n_inputs)</code>，<code>n_inputs</code>是层的输入数。这篇论文说，使用这个方法可以显著提升 GAN 使用 RMSProp、Adam 和其它适应梯度优化器时的性能。事实上，这些优化器用估计标准差（见第 11 章）归一化了梯度更新，所以有较大动态范围的参数需要更长时间训练，而较小动态范围的参数可能更新过快，会导致不稳定。通过缩放模型的部分参数，可以保证参数的动态范围在训练过程中一致，可以用相同的速度学习。这样既加速了训练，也做到了稳定。</p>
</li>
<li><p>像素级归一化层</p>
<p>生成器的每个卷积层之后添加。它能归一化每个激活函数，基于相同图片相同位置的所有激活，而且跨通道（除以平均激活平方的平方根）。在 TensorFlow 的代码中，这是<code>inputs / tf.sqrt(tf.reduce_mean(tf.square(X), axis=-1, keepdims=True) + 1e-8)</code>（平滑项<code>1e-8</code>用于避免零除）。这种方法可以避免生成器和判别器的过分竞争导致的激活爆炸。</p>
</li>
</ul>
<p>使用所有这些方法，作者制作出了<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fhoml.info%2Fprogandemo" target="_blank">非常逼真的人脸图片</a>。但如何给“逼真”下定义呢？GAN 的评估时一大挑战：尽管可以自动评估生成图片的散度，判断质量要棘手和主观的多。一种方法是让人来打分，但成本高且耗时。因此作者建议比较生成图和训练图的局部图片结构，在各个层次比较。这个想法使他们创造出了另一个突破性的成果：StyleGAN。</p>
<h3 id="stylegan">StyleGAN</h3>
<p>相同的 Nvidia 团队在 2018 年的一篇<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fhoml.info%2Fstylegan" target="_blank">论文</a>中提出了高性能的高清图片生成架构，StyleGAN。作者在生成器中使用了风格迁移方法，使生成的图片和训练图片在每个层次，都有相同的局部结构，极大提升了图片的质量。判别器和损失函数没有变动，只修改了生成器。StyleGAN 包含两个网络（见图 17-20）：</p>
<ul>
<li><p>映射网络</p>
<p>一个八层的 MLP，将潜在表征<code>z</code>（即，编码）映射为向量<code>w</code>。向量然后传给仿射变换（即，没有激活函数的紧密层，用图 17-20 中的框 A 表示），输出许多向量。这些向量在不同级别控制着生成图片的风格，从细粒度纹理（比如，头发颜色）到高级特征（比如，成人或孩子）。总而言之，映射网络将编码变为许多风格向量。</p>
</li>
<li><p>合成网络</p>
<p>负责生成图片。它有一个固定的学好的输入（这个输入在训练之后是不变的，但在训练中被反向传播更新）。和之前一样，合成网络使用多个卷积核上采样层处理输入，但有两处不同：首先，输入和所有卷积层的输出（在激活函数之前）都添加了噪音。第二，每个噪音层的后面是一个适应实例归一化（AdaIN）层：它独立标准化每个特征映射（减去平均值，除以标准差），然后使用风格向量确定每个特征映射的缩放和偏移（风格向量对每个特征映射包含一个缩放和一个偏置项）。</p>
</li>
</ul>
<p><img src="img/16e64eb0ee762551f526e51a257df423.png" alt=""></img></p>
<p>图 17-20 StyleGAN 的生成器架构（StyleGAN 论文的图 1 的一部分）</p>
<p>在编码层独立添加噪音非常重要。图片的一些部分是很随机的，比如雀斑和头发的确切位置。在早期的 GAN 中，这个随机性要么来自编码，要么是生成器的一些伪噪音。如果来自编码，意味着生成器要用编码的很重要的一部分来存储噪音：这样会非常浪费。另外，噪音会在网络中流动，直到生成器的最后一层：这是一种没有必要的约束，会显著减慢训练。最后，因为噪音的存在，会出现一些视觉伪影。如果是生成器来制造伪噪音，噪音可能不够真实，造成更多的视觉伪影。另外，用生成器的一部分权重来生成伪噪音，这也是一种浪费。通过添加额外的噪音输入，可以避免所有这些问题；GAN 可以利用噪音，给图片的每个部分添加随机量。</p>
<p>添加的噪音在每个级别都不同。每个噪音输入包含一个单独的包含高斯噪音的特征映射，广播到所有特征映射上（给定级别），然后在添加前用每个特征的缩放因子缩放（这是图 17-20 的框 B）。</p>
<p>最后，StyleGAN 使用了一种称为混合正则（或风格混合）的方法，生成图的一定比例使用两个编码来生成。特别的，编码<code>c[1]</code>和<code>c[2]</code>发送给映射网络，得到两个风格向量<code>w[1]</code>和<code>w[2]</code>。然后合成网络使用风格<code>w[1]</code>生成第一级，用<code>w[2]</code>生成其余的。级的选取是随机的。这可以防止模型认为临近的级是有关联的，会导致 GAN 的局部性，每个风格向量只会影响生成图的有限数量的特性。</p>
<p>GAN 的种类如此之多，用一本书才能介绍全。希望这里的内容可以告诉你 GAN 的主要观点，以及继续学习的动力。如果你对数学概念掌握不好，可以看看网上的博客。然后就可以创建自己的 GAN 了，如果一开始碰到问题，千万别气馁：有问题是正常的，通常要好好练习，才能掌握好。如果对实现细节不明白，可以看看别人的 Keras 和 TensorFlow 实现。事实上，如果你只是想快速获得一些经验的结果，可以使用预训练模型（例如，存在适用于 Keras 的 StyleGAN 预训练模型）。</p>
<p>下一章会介绍深度学习的另一领域：深度强化学习。</p>
<h2 id="练习">练习</h2>
<ol>
<li><p>自编码器主要用来做什么？</p>
</li>
<li><p>假设你想训练一个分类器，有许多未打标签的训练数据，只有一千多打了标签的数据。如何使用自编码器来解决这个问题？</p>
</li>
<li><p>如果自编码器完美重建了输入，它一定是个好的自编码器吗？如何评估自编码器的表现？</p>
</li>
<li><p>自编码器的欠完成和过完成是什么？超欠完成的风险是什么？过完成的风险是什么？</p>
</li>
<li><p>如何将栈式自编码器的权重连起来？这么做的意义是什么？</p>
</li>
<li><p>什么是生成式模型？可以举出生成式自编码器的例子吗？</p>
</li>
<li><p>GAN 是什么？可以用于什么任务？</p>
</li>
<li><p>训练 GAN 的难点是什么？</p>
</li>
<li><p>用去噪音自编码器预训练一个图片分类器。可以使用 MNIST，或是更复杂的图片数据集，比如 CIFAR10。不管用的是什么数据集，遵循下面的步骤：</p>
<ul>
<li><p>将数据集分成训练集和测试集。在完整训练集上，训练一个深度去噪音自编码器。</p>
</li>
<li><p>检查图片正确重建了。可视化最激活编码层神经元的图片。</p>
</li>
<li><p>搭建一个分类 DNN，使用自编码器的浅层。用训练集中的 500 张图片来训练。然后判断预训练是否提升了性能？</p>
</li>
</ul>
</li>
<li><p>用刚才选择的数据集，训练一个变分自编码器。用它来生成图片。或者，用一个没有标签的数据集，来生成新样本。</p>
</li>
<li><p>训练一个 DCGAN 来处理选择的数据集，生成新图片。添加经验接力，看看它是否有作用。再将其变为一个条件 GAN，可以控制生成的类。</p>
</li>
</ol>
<p>参考答案见附录 A。</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="16.html" class="navigation navigation-prev " aria-label="Previous page: 十六、使用 RNN 和注意力机制进行自然语言处理">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="18.html" class="navigation navigation-next " aria-label="Next page: 十八、强化学习">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"十七、使用自编码器和 GAN 做表征学习和生成式学习","level":"1.18","depth":1,"next":{"title":"十八、强化学习","level":"1.19","depth":1,"path":"18.md","ref":"./18.md","articles":[]},"previous":{"title":"十六、使用 RNN 和注意力机制进行自然语言处理","level":"1.17","depth":1,"path":"16.md","ref":"./16.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"17.md","mtime":"2024-08-23T21:05:56.818Z","type":"markdown"},"gitbook":{"version":"5.1.4","time":"2024-08-24T12:29:35.670Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

