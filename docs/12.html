
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <title>十二、使用 TensorFlow 自定义模型并训练 · HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 5.1.4">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/@honkit/honkit-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, user-scalable=yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="13.html" />
    
    
    <link rel="prev" href="11.html" />
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="0.html">
            
                <a href="0.html">
            
                    
                    零、前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="1.html">
            
                <a href="1.html">
            
                    
                    一、机器学习概览
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="2.html">
            
                <a href="2.html">
            
                    
                    二、端到端的机器学习项目
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="3.html">
            
                <a href="3.html">
            
                    
                    三、分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="4.html">
            
                <a href="4.html">
            
                    
                    四、训练模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="5.html">
            
                <a href="5.html">
            
                    
                    五、支持向量机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="6.html">
            
                <a href="6.html">
            
                    
                    六、决策树
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="7.html">
            
                <a href="7.html">
            
                    
                    七、集成学习和随机森林
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="8.html">
            
                <a href="8.html">
            
                    
                    八、降维
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="10.html">
            
                <a href="10.html">
            
                    
                    十、使用 Keras 搭建人工神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12" data-path="11.html">
            
                <a href="11.html">
            
                    
                    十一、训练深度神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.13" data-path="12.html">
            
                <a href="12.html">
            
                    
                    十二、使用 TensorFlow 自定义模型并训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14" data-path="13.html">
            
                <a href="13.html">
            
                    
                    十三、使用 TensorFlow 加载和预处理数据
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15" data-path="14.html">
            
                <a href="14.html">
            
                    
                    十四、使用卷积神经网络实现深度计算机视觉
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16" data-path="15.html">
            
                <a href="15.html">
            
                    
                    十五、使用 RNN 和 CNN 处理序列
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17" data-path="16.html">
            
                <a href="16.html">
            
                    
                    十六、使用 RNN 和注意力机制进行自然语言处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.18" data-path="17.html">
            
                <a href="17.html">
            
                    
                    十七、使用自编码器和 GAN 做表征学习和生成式学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.19" data-path="18.html">
            
                <a href="18.html">
            
                    
                    十八、强化学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.20" data-path="19.html">
            
                <a href="19.html">
            
                    
                    十九、规模化训练和部署 TensorFlow 模型
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Published with HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >十二、使用 TensorFlow 自定义模型并训练</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="十二、使用-tensorflow-自定义模型并训练">十二、使用 TensorFlow 自定义模型并训练</h1>
<blockquote>
<p>译者：<a href="https://www.jianshu.com/u/130f76596b02" target="_blank">@SeanCheney</a></p>
</blockquote>
<p>目前为止，我们只是使用了 TensorFlow 的高级 API —— <code>tf.keras</code>，它的功能很强大：搭建了各种神经网络架构，包括回归、分类网络、Wide &amp; Deep 网络、自归一化网络，使用了各种方法，包括批归一化、丢弃和学习率调度。事实上，你在实际案例中 95% 碰到的情况只需要<code>tf.keras</code>就足够了（和<code>tf.data</code>，见第 13 章）。现在来深入学习 TensorFlow 的低级 Python API。当你需要实现自定义损失函数、自定义标准、层、模型、初始化器、正则器、权重约束时，就需要低级 API 了。甚至有时需要全面控制训练过程，例如使用特殊变换或对约束梯度时。这一章就会讨论这些问题，还会学习如何使用 TensorFlow 的自动图生成特征提升自定义模型和训练算法。首先，先来快速学习下 TensorFlow。</p>
<blockquote>
<p>笔记：TensorFlow 2.0（beta）是 2019 年六月发布的，相比前代更易使用。本书第一版使用的是 TF 1，这一版使用的是 TF 2。</p>
</blockquote>
<h2 id="tensorflow-速览">TensorFlow 速览</h2>
<p>TensorFlow 是一个强大的数值计算库，特别适合做和微调大规模机器学习（但也可以用来做其它的重型计算）。TensorFlow 是谷歌大脑团队开发的，支持了谷歌的许多大规模服务，包括谷歌云对话、谷歌图片和谷歌搜索。TensorFlow 是 2015 年 11 月开源的，（按文章引用、公司采用、GitHub 星数）是目前最流行的深度学习库。无数的项目是用 TensorFlow 来做各种机器学习任务，包括图片分类、自然语言处理、推荐系统和时间序列预测。TensorFlow 提供的功能如下：</p>
<ul>
<li><p>TensorFlow 的核心与 NumPy 很像，但 TensorFlow 支持 GPU；</p>
</li>
<li><p>TensorFlow 支持（多设备和服务器）分布式计算；</p>
</li>
<li><p>TensorFlow 使用了即时 JIT 编译器对计算速度和内存使用优化。编译器的工作是从 Python 函数提取出计算图，然后对计算图优化（比如剪切无用的节点），最后高效运行（比如自动并行运行独立任务）；</p>
</li>
<li><p>计算图可以导出为迁移形式，因此可以在一个环境中训练一个 TensorFlow 模型（比如使用 Python 或 Linux），然后在另一个环境中运行（比如在安卓设备上用 Java 运行）；</p>
</li>
<li><p>TensorFlow 实现了自动微分，并提供了一些高效的优化器，比如 RMSProp 和 NAdam，因此可以容易的最小化各种损失函数。</p>
</li>
</ul>
<p>基于上面这些特点，TensorFlow 还提供了许多其他功能：最重要的是<code>tf.keras</code>，还有数据加载和预处理操作（<code>tf.data</code>，<code>tf.io</code>等等），图片处理操作（<code>tf.image</code>），信号处理操作（<code>tf.signal</code>），等等（图 12-1 总结了 TensorFlow 的 Python API）</p>
<p><img src="img/3d01200878f6c6d7033359da8291d199.png" alt=""></img></p>
<p>图 12-1 TensorFlow 的 Python API</p>
<blockquote>
<p>提示：这一章会介绍 TensorFlow API 的多个包和函数，但来不及介绍全部，所以读者最好自己花点时间好好看看 API。TensorFlow 的 API 十分丰富，且文档详实。</p>
</blockquote>
<p>TensorFlow 的低级操作都是用高效的 C++ 实现的。许多操作有多个实现，称为<code>核</code>：每个核对应一个具体的设备型号，比如 CPU、GPU，甚至 TPU（张量处理单元）。GPU 通过将任务分成小块，在多个 GPU 线程中并行运行，可以极大提高提高计算的速度。TPU 更快：TPU 是自定义的 ASIC 芯片，专门用来做深度学习运算的（第 19 章会讨论适合使用 GPU 和 TPU）。</p>
<p>TensorFlow 的架构见图 12-2。大多数时候你的代码使用高级 API 就够了（特别是<code>tf.keras</code>和<code>tf.data</code>），但如果需要更大的灵活性，就需要使用低级 Python API，来直接处理张量。TensorFlow 也支持其它语言的 API。任何情况下，甚至是跨设备和机器的情况下，TensorFlow 的执行引擎都会负责高效运行。</p>
<p><img src="img/ed0c3d9af331e1630b0d6051a6899d01.png" alt=""></img></p>
<p>图 12-2 TensorFlow 的架构</p>
<p>TensorFlow 不仅可以运行在 Windows、Linux 和 macOS 上，也可以运行在移动设备上（使用 TensorFlow Lite），包括 iOS 和安卓（见第 19 章）。如果不想使用 Python API，还可以使用 C++、Java、Go 和 Swift 的 API。甚至还有 JavaScript 的实现 TensorFlow.js，它可以直接在浏览器中运行。</p>
<p>TensorFlow 不只有这些库。TensorFlow 处于一套可扩展的生态系统库的核心位置。首先，TensorBoard 可以用来可视化。其次，TensorFlow Extended（TFX），是谷歌推出的用来生产化的库，包括：数据确认、预处理、模型分析和服务（使用 TF Serving，见第 19 章）。谷歌的 TensorFlow Hub 上可以方便下载和复用预训练好的神经网络。你还可以从 TensorFlow 的 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Fmodels%2F" target="_blank">model garden</a> 获取许多神经网络架构，其中一些是预训练好的。<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.tensorflow.org%2Fresources" target="_blank">TensorFlow Resources</a> 和<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fjtoy%2Fawesome-tensorflow" target="_blank">这个页面</a>上有更多的资源。你可以在 GitHub 上找到数百个 TensorFlow 项目，无论干什么都可以方便地找到现成的代码。</p>
<blockquote>
<p>提示：越来越多的 ML 论文都附带了实现过程，一些甚至带有预训练模型。可以在<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpaperswithcode.com%2F" target="_blank">这里</a>找到。</p>
</blockquote>
<p>最后，TensorFlow 有一支热忱满满的开发者团队，也有庞大的社区。要是想问技术问题，可以去<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fstackoverflow.com%2F" target="_blank">这里</a>
，问题上打上 tensorflow 和 python 标签。还可以在 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow" target="_blank">GitHub</a> 上提 bug 和新功能。一般的讨论可以去<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgroups.google.com%2Fa%2Ftensorflow.org%2Fforum%2F" target="_blank">谷歌群组</a>。</p>
<p>下面开始写代码！</p>
<h2 id="像-numpy-一样使用-tensorflow">像 NumPy 一样使用 TensorFlow</h2>
<p>TensorFlow 的 API 是围绕张量（tensor）展开的，从一个操作流动（flow）到另一个操作，所以名字叫做 TensorFlow。张量通常是一个多维数组（就像 NumPy 的<code>ndarray</code>），但也可以是标量（即简单值，比如 42）。张量对于自定义的损失函数、标准、层等等非常重要，接下来学习如何创建和操作张量。</p>
<h3 id="张量和运算">张量和运算</h3>
<p>使用<code>tf.constant()</code>创建张量。例如，下面的张量表示的是两行三列的浮点数矩阵：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>tf.constant([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>], [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]]) <span class="hljs-comment"># matrix</span>
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">0</span>, shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=float32, numpy=
array([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
       [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]], dtype=float32)&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>tf.constant(<span class="hljs-number">42</span>) <span class="hljs-comment"># 标量</span>
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">1</span>, shape=(), dtype=int32, numpy=<span class="hljs-number">42</span>&gt;
</code></pre>
<p>就像<code>ndarray</code>一样，<code>tf.Tensor</code>也有形状和数据类型（<code>dtype</code>）：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>t = tf.constant([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>], [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>t.shape
TensorShape([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>t.dtype
tf.float32
</code></pre>
<p>索引和 NumPy 中很像：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>t[:, <span class="hljs-number">1</span>:]
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">5</span>, shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
       [<span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]], dtype=float32)&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>t[..., <span class="hljs-number">1</span>, tf.newaxis]
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">15</span>, shape=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), dtype=float32, numpy=
array([[<span class="hljs-number">2.</span>],
       [<span class="hljs-number">5.</span>]], dtype=float32)&gt;
</code></pre>
<p>最重要的，所有张量运算都可以执行：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>t + <span class="hljs-number">10</span>
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">18</span>, shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=float32, numpy=
array([[<span class="hljs-number">11.</span>, <span class="hljs-number">12.</span>, <span class="hljs-number">13.</span>],
       [<span class="hljs-number">14.</span>, <span class="hljs-number">15.</span>, <span class="hljs-number">16.</span>]], dtype=float32)&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>tf.square(t)
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">20</span>, shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=float32, numpy=
array([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">9.</span>],
       [<span class="hljs-number">16.</span>, <span class="hljs-number">25.</span>, <span class="hljs-number">36.</span>]], dtype=float32)&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>t @ tf.transpose(t)
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">24</span>, shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[<span class="hljs-number">14.</span>, <span class="hljs-number">32.</span>],
       [<span class="hljs-number">32.</span>, <span class="hljs-number">77.</span>]], dtype=float32)&gt;
</code></pre>
<p>可以看到，<code>t + 10</code>等同于调用<code>tf.add(t, 10)</code>，<code>-</code>和<code>*</code>也支持。<code>@</code>运算符是在 Python3.5 中出现的，用于矩阵乘法，等同于调用函数<code>tf.matmul()</code>。</p>
<p>可以在 tf 中找到所有基本的数学运算（<code>tf.add()</code>、<code>tf.multiply()</code>、<code>tf.square()</code>、<code>tf.exp()</code>、<code>tf.sqrt()</code>），以及 NumPy 中的大部分运算（比如<code>tf.reshape()</code>、<code>tf.squeeze()</code>、<code>tf.tile()</code>）。一些 tf 中的函数与 NumPy 中不同，例如，<code>tf.reduce_mean()</code>、<code>tf.reduce_sum()</code>、<code>tf.reduce_max()</code>、<code>tf.math.log()</code>等同于<code>np.mean()</code>、<code>np.sum()</code>、<code>np.max()</code>和<code>np.log()</code>。当函数名不同时，通常都是有原因的。例如，TensorFlow 中必须使用<code>tf.transpose(t)</code>，不能像 NumPy 中那样使用<code>t.T</code>。原因是函数<code>tf.transpose(t)</code>所做的和 NumPy 的属性<code>T</code>并不完全相同：在 TensorFlow 中，是使用转置数据的复制来生成张量的，而在 NumPy 中，<code>t.T</code>是数据的转置视图。相似的，<code>tf.reduce_sum()</code>操作之所以这么命名，是因为它的 GPU 核（即 GPU 实现）所采用的归约算法不能保证元素相加的顺序，因为 32 位的浮点数精度有限，每次调用的结果可能会有细微的不同。<code>tf.reduce_mean()</code>也是这样（<code>tf.reduce_max()</code>结果是确定的）。</p>
<blockquote>
<p>笔记：许多函数和类都有假名。比如，<code>tf.add()</code>和<code>tf.math.add()</code>是相同的。这可以让 TensorFlow 对于最常用的操作有简洁的名字，同时包可以有序安置。</p>
<p>Keras 的低级 API
Keras API 有自己的低级 API，位于<code>keras.backend</code>，包括：函数<code>square()</code>、<code>exp()</code>、<code>sqrt()</code>。在<code>tf.keras</code>中，这些函数通常通常只是调用对应的 TensorFlow 操作。如果你想写一些可以迁移到其它 Keras 实现上，就应该使用这些 Keras 函数。但是这些函数不多，所以这本书里就直接使用 TensorFlow 的运算了。下面是一个简单的使用了<code>keras.backend</code>的例子，简记为<code>k</code>：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-meta">&gt;&gt;&gt; </span>K = keras.backend
<span class="hljs-meta">&gt;&gt;&gt; </span>K.square(K.transpose(t)) + <span class="hljs-number">10</span>
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">39</span>, shape=(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[<span class="hljs-number">11.</span>, <span class="hljs-number">26.</span>],
       [<span class="hljs-number">14.</span>, <span class="hljs-number">35.</span>],
       [<span class="hljs-number">19.</span>, <span class="hljs-number">46.</span>]], dtype=float32)&gt;
</code></pre>
</blockquote>
<h3 id="张量和-numpy">张量和 NumPy</h3>
<p>张量和 NumPy 融合地非常好：使用 NumPy 数组可以创建张量，张量也可以创建 NumPy 数组。可以在 NumPy 数组上运行 TensorFlow 运算，也可以在张量上运行 NumPy 运算：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([<span class="hljs-number">2.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>tf.constant(a)
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">111</span>, shape=(<span class="hljs-number">3</span>,), dtype=float64, numpy=array([<span class="hljs-number">2.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>])&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>t.numpy() <span class="hljs-comment"># 或 np.array(t)</span>
array([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
       [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]], dtype=float32)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf.square(a)
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">116</span>, shape=(<span class="hljs-number">3</span>,), dtype=float64, numpy=array([<span class="hljs-number">4.</span>, <span class="hljs-number">16.</span>, <span class="hljs-number">25.</span>])&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>np.square(t)
array([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">9.</span>],
       [<span class="hljs-number">16.</span>, <span class="hljs-number">25.</span>, <span class="hljs-number">36.</span>]], dtype=float32)
</code></pre>
<blockquote>
<p>警告：NumPy 默认使用 64 位精度，TensorFlow 默认用 32 位精度。这是因为 32 位精度通常对于神经网络就足够了，另外运行地更快，使用的内存更少。因此当你用 NumPy 数组创建张量时，一定要设置<code>dtype=tf.float32</code>。</p>
</blockquote>
<h3 id="类型转换">类型转换</h3>
<p>类型转换对性能的影响非常大，并且如果类型转换是自动完成的，不容易被注意到。为了避免这样，TensorFlow 不会自动做任何类型转换：只是如果用不兼容的类型执行了张量运算，TensorFlow 就会报异常。例如，不能用浮点型张量与整数型张量相加，也不能将 32 位张量与 64 位张量相加：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>tf.constant(<span class="hljs-number">2.</span>) + tf.constant(<span class="hljs-number">40</span>)
Traceback[...]InvalidArgumentError[...]expected to be a <span class="hljs-built_in">float</span>[...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tf.constant(<span class="hljs-number">2.</span>) + tf.constant(<span class="hljs-number">40.</span>, dtype=tf.float64)
Traceback[...]InvalidArgumentError[...]expected to be a double[...]
</code></pre>
<p>这点可能一开始有点恼人，但是有其存在的理由。如果真的需要转换类型，可以使用<code>tf.cast()</code>：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>t2 = tf.constant(<span class="hljs-number">40.</span>, dtype=tf.float64)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf.constant(<span class="hljs-number">2.0</span>) + tf.cast(t2, tf.float32)
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">136</span>, shape=(), dtype=float32, numpy=<span class="hljs-number">42.0</span>&gt;
</code></pre>
<h3 id="变量">变量</h3>
<p>到目前为止看到的<code>tf.Tensor</code>值都是不能修改的。意味着不能使用常规张量实现神经网络的权重，因为权重必须要能被反向传播调整。另外，其它的参数也需要随着时间调整（比如，动量优化器要跟踪过去的梯度）。此时需要的是<code>tf.Variable</code>：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>v = tf.Variable([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>], [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>v
&lt;tf.Variable <span class="hljs-string">'Variable:0'</span> shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>) dtype=float32, numpy=
array([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
       [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]], dtype=float32)&gt;
</code></pre>
<p><code>tf.Variable</code>和<code>tf.Tensor</code>很像：可以运行同样的运算，可以配合 NumPy 使用，也要注意类型。可以使用<code>assign()</code>方法对其就地修改（或<code>assign_add()</code>、<code>assign_sub()</code>）。使用切片的<code>assign()</code>方法可以修改独立的切片（直接赋值行不通），或使用<code>scatter_update()</code>、<code>scatter_nd_update()</code>方法：</p>
<pre><code class="lang-py">v.assign(<span class="hljs-number">2</span> * v)           <span class="hljs-comment"># =&gt; [[2., 4., 6.], [8., 10., 12.]]</span>
v[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>].assign(<span class="hljs-number">42</span>)        <span class="hljs-comment"># =&gt; [[2., 42., 6.], [8., 10., 12.]]</span>
v[:, <span class="hljs-number">2</span>].assign([<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>])  <span class="hljs-comment"># =&gt; [[2., 42., 0.], [8., 10., 1.]]</span>
v.scatter_nd_update(indices=[[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]], updates=[<span class="hljs-number">100.</span>, <span class="hljs-number">200.</span>])
                          <span class="hljs-comment"># =&gt; [[100., 42., 0.], [8., 10., 200.]]</span>
</code></pre>
<blockquote>
<p>笔记：在实践中，很少需要手动创建变量，因为 Keras 有<code>add_weight()</code>方法可以自动来做。另外，模型参数通常会直接通过优化器更新，因此很少需要手动更新。</p>
</blockquote>
<h3 id="其它数据结构">其它数据结构</h3>
<p>TensorFlow 还支持其它几种数据结构，如下（可以参考笔记本的<code>Tensors and Operations</code>部分，或附录的 F）：</p>
<p>稀疏张量（<code>tf.SparseTensor</code>）
高效表示含有许多 0 的张量。<code>tf.sparse</code>包含有对稀疏张量的运算。</p>
<p>张量数组（<code>tf.TensorArray</code>）
是张量的列表。有默认固定大小，但也可以做成动态的。列表中的张量必须形状相同，数据类型也相同。</p>
<p>嵌套张量（<code>tf.RaggedTensor</code>）
张量列表的静态列表，张量的形状和数据结构相同。<code>tf.ragged</code>包里有嵌套张量的运算。</p>
<p>字符串张量
类型是<code>tf.string</code>的常规张量，是字节串而不是 Unicode 字符串，因此如果你用 Unicode 字符串（比如，Python3 字符串<code>café</code>）创建了一个字符串张量，就会自动被转换为 UTF-8（<code>b"caf\xc3\xa9"</code>）。另外，也可以用<code>tf.int32</code>类型的张量表示 Unicode 字符串，其中每项表示一个 Unicode 码（比如，<code>[99, 97, 102, 233]</code>）。<code>tf.strings</code>包里有字节串和 Unicode 字符串的运算，以及二者转换的运算。要注意<code>tf.string</code>是原子性的，也就是说它的长度不出现在张量的形状中，一旦将其转换成了 Unicode 张量（即，含有 Unicode 码的<code>tf.int32</code>张量），长度才出现在形状中。</p>
<p>集合
表示为常规张量（或稀疏张量）。例如<code>tf.constant([[1, 2], [3, 4]])</code>表示两个集合{1, 2}和{3, 4}。通常，用张量的最后一个轴的向量表示集合。集合运算可以用<code>tf.sets</code>包。</p>
<p>队列
用来在多个步骤之间保存张量。TensorFlow 提供了多种队列。先进先出（FIFO）队列<code>FIFOQueue</code>，优先级队列<code>PriorityQueue</code>，随机队列<code>RandomShuffleQueue</code>，通过填充的不同形状的批次项队列<code>PaddingFIFOQueue</code>。这些队列都在<code>tf.queue</code>包中。</p>
<p>有了张量、运算、变量和各种数据结构，就可以开始自定义模型和训练算法啦！</p>
<h2 id="自定义模型和训练算法">自定义模型和训练算法</h2>
<p>先从简单又常见的任务开始，创建一个自定义的损失函数。</p>
<h3 id="自定义损失函数">自定义损失函数</h3>
<p>假如你想训练一个回归模型，但训练集有噪音。你当然可以通过清除或修正异常值来清理数据集，但是这样还不够：数据集还是有噪音。此时，该用什么损失函数呢？均方差可能对大误差惩罚过重，导致模型不准确。均绝对值误差不会对异常值惩罚过重，但训练可能要比较长的时间才能收敛，训练模型也可能不准确。此时使用 Huber 损失（第 10 章介绍过）就比 MSE 好多了。目前官方 Keras API 中没有 Huber 损失，但<code>tf.keras</code>有（使用类<code>keras.losses.Huber</code>的实例）。就算<code>tf.keras</code>没有，实现也不难！只需创建一个函数，参数是标签和预测值，使用 TensorFlow 运算计算每个实例的损失：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">huber_fn</span>(<span class="hljs-params">y_true, y_pred</span>):
    error = y_true - y_pred
    is_small_error = tf.<span class="hljs-built_in">abs</span>(error) &lt; <span class="hljs-number">1</span>
    squared_loss = tf.square(error) / <span class="hljs-number">2</span>
    linear_loss  = tf.<span class="hljs-built_in">abs</span>(error) - <span class="hljs-number">0.5</span>
    <span class="hljs-keyword">return</span> tf.where(is_small_error, squared_loss, linear_loss)
</code></pre>
<blockquote>
<p>警告：要提高性能，应该像这个例子使用向量。另外，如果想利用 TensorFlow 的图特性，则只能使用 TensorFlow 运算。</p>
</blockquote>
<p>最好返回一个包含实例的张量，其中每个实例都有一个损失，而不是返回平均损失。这么做的话，Keras 可以在需要时，使用类权重或样本权重（见第 10 章）。</p>
<p>现在，编译 Keras 模型时，就可以使用 Huber 损失来训练了：</p>
<pre><code class="lang-py">model.<span class="hljs-built_in">compile</span>(loss=huber_fn, optimizer=<span class="hljs-string">"nadam"</span>)
model.fit(X_train, y_train, [...])
</code></pre>
<p>仅此而已！对于训练中的每个批次，Keras 会调用函数<code>huber_fn()</code>计算损失，用损失来做梯度下降。另外，Keras 会从一开始跟踪总损失，并展示平均损失。</p>
<p>在保存这个模型时，这个自定义损失会发生什么呢？</p>
<h3 id="保存并加载包含自定义组件的模型">保存并加载包含自定义组件的模型</h3>
<p>因为 Keras 可以保存函数名，保存含有自定义损失函数的模型也不成问题。当加载模型时，你需要提供一个字典，这个字典可以将函数名和真正的函数映射起来。一般说来，当加载一个含有自定义对象的模型时，你需要将名字映射到对象上：</p>
<pre><code class="lang-py">model = keras.models.load_model(<span class="hljs-string">"my_model_with_a_custom_loss.h5"</span>,
                                custom_objects={<span class="hljs-string">"huber_fn"</span>: huber_fn})
</code></pre>
<p>对于刚刚的代码，在 -1 和 1 之间的误差被认为是“小”误差。如果要改变阈值呢？一个解决方法是创建一个函数，它可以产生一个可配置的损失函数：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_huber</span>(<span class="hljs-params">threshold=<span class="hljs-number">1.0</span></span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">huber_fn</span>(<span class="hljs-params">y_true, y_pred</span>):
        error = y_true - y_pred
        is_small_error = tf.<span class="hljs-built_in">abs</span>(error) &lt; threshold
        squared_loss = tf.square(error) / <span class="hljs-number">2</span>
        linear_loss  = threshold * tf.<span class="hljs-built_in">abs</span>(error) - threshold**<span class="hljs-number">2</span> / <span class="hljs-number">2</span>
        <span class="hljs-keyword">return</span> tf.where(is_small_error, squared_loss, linear_loss)
    <span class="hljs-keyword">return</span> huber_fn
model.<span class="hljs-built_in">compile</span>(loss=create_huber(<span class="hljs-number">2.0</span>), optimizer=<span class="hljs-string">"nadam"</span>)
</code></pre>
<p>但在保存模型时，<code>threshold</code>不能被保存。这意味在加载模型时（注意，给 Keras 的函数名是<code>Huber_fn</code>，不是创造这个函数的函数名），必须要指定<code>threshold</code>的值：</p>
<pre><code class="lang-py">model = keras.models.load_model(<span class="hljs-string">"my_model_with_a_custom_loss_threshold_2.h5"</span>,
                                custom_objects={<span class="hljs-string">"huber_fn"</span>: create_huber(<span class="hljs-number">2.0</span>)})
</code></pre>
<p>要解决这个问题，可以创建一个<code>keras.losses.Loss</code>类的子类，然后实现<code>get_config()</code>方法：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">HuberLoss</span>(keras.losses.Loss):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, threshold=<span class="hljs-number">1.0</span>, **kwargs</span>):
        <span class="hljs-variable language_">self</span>.threshold = threshold
        <span class="hljs-built_in">super</span>().__init__(**kwargs)
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, y_true, y_pred</span>):
        error = y_true - y_pred
        is_small_error = tf.<span class="hljs-built_in">abs</span>(error) &lt; <span class="hljs-variable language_">self</span>.threshold
        squared_loss = tf.square(error) / <span class="hljs-number">2</span>
        linear_loss  = <span class="hljs-variable language_">self</span>.threshold * tf.<span class="hljs-built_in">abs</span>(error) - <span class="hljs-variable language_">self</span>.threshold**<span class="hljs-number">2</span> / <span class="hljs-number">2</span>
        <span class="hljs-keyword">return</span> tf.where(is_small_error, squared_loss, linear_loss)
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_config</span>(<span class="hljs-params">self</span>):
        base_config = <span class="hljs-built_in">super</span>().get_config()
        <span class="hljs-keyword">return</span> {**base_config, <span class="hljs-string">"threshold"</span>: <span class="hljs-variable language_">self</span>.threshold}
</code></pre>
<blockquote>
<p>警告：Keras API 目前只使用子类来定义层、模型、调回和正则器。如果使用子类创建其它组件（比如损失、指标、初始化器或约束），它们不能迁移到其它 Keras 实现上。可能 Keras API 经过更新，就会支持所有组件了。</p>
</blockquote>
<p>逐行看下这段代码：</p>
<ul>
<li><p>构造器接收<code>**kwargs</code>，并将其传递给父构造器，父构造器负责处理超参数：损失的<code>name</code>，要使用的、用于将单个实例的损失汇总的<code>reduction</code>算法。默认情况下是<code>"sum_over_batch_size"</code>，意思是损失是各个实例的损失之和，如果有样本权重，则做权重加权，再除以批次大小（不是除以权重之和，所以不是加权平均）。其它可能的值是<code>"sum"</code>和<code>None</code>。</p>
</li>
<li><p><code>call()</code>方法接受标签和预测值，计算所有实例的损失，并返回。</p>
</li>
<li><p><code>get_config()</code>方法返回一个字典，将每个超参数映射到值上。它首先调用父类的<code>get_config()</code>方法，然后将新的超参数加入字典（<code>{**x}</code>语法是 Python 3.5 引入的）。</p>
</li>
</ul>
<p>当编译模型时，可以使用这个类的实例：</p>
<pre><code class="lang-py">model.<span class="hljs-built_in">compile</span>(loss=HuberLoss(<span class="hljs-number">2.</span>), optimizer=<span class="hljs-string">"nadam"</span>)
</code></pre>
<p>保存模型时，阈值会一起保存；加载模型时，只需将类名映射到具体的类上：</p>
<pre><code class="lang-py">model = keras.models.load_model(<span class="hljs-string">"my_model_with_a_custom_loss_class.h5"</span>,
                                custom_objects={<span class="hljs-string">"HuberLoss"</span>: HuberLoss})
</code></pre>
<p>保存模型时，Keras 调用损失实例的<code>get_config()</code>方法，将配置以 JSON 的形式保存在 HDF5 中。当加载模型时，会调用<code>HuberLoss</code>类的<code>from_config()</code>方法：这个方法是父类<code>Loss</code>实现的，创建一个类<code>Loss</code>的实例，将<code>**config</code>传递给构造器。</p>
<h3 id="自定义激活函数、初始化器、正则器和约束">自定义激活函数、初始化器、正则器和约束</h3>
<p>Keras 的大多数功能，比如损失、正则器、约束、初始化器、指标、激活函数、层，甚至是完整的模型，都可以用相似的方法做自定义。大多数时候，需要写一个简单的函数，带有合适的输入和输出。下面的例子是自定义激活函数（等价于<code>keras.activations.softplus()</code>或<code>tf.nn.softplus()</code>），自定义 Glorot 初始化器（等价于<code>keras.initializers.glorot_normal()</code>），自定义<code>ℓ1</code>正则化器（等价于<code>keras.regularizers.l1(0.01)</code>），可以保证权重都是正值的自定义约束（等价于<code>equivalent to keras.constraints.nonneg()</code>或<code>tf.nn.relu()</code>）：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_softplus</span>(<span class="hljs-params">z</span>): <span class="hljs-comment"># return value is just tf.nn.softplus(z)</span>
    <span class="hljs-keyword">return</span> tf.math.log(tf.exp(z) + <span class="hljs-number">1.0</span>)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">my_glorot_initializer</span>(<span class="hljs-params">shape, dtype=tf.float32</span>):
    stddev = tf.sqrt(<span class="hljs-number">2</span>\. / (shape[<span class="hljs-number">0</span>] + shape[<span class="hljs-number">1</span>]))
    <span class="hljs-keyword">return</span> tf.random.normal(shape, stddev=stddev, dtype=dtype)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">my_l1_regularizer</span>(<span class="hljs-params">weights</span>):
    <span class="hljs-keyword">return</span> tf.reduce_sum(tf.<span class="hljs-built_in">abs</span>(<span class="hljs-number">0.01</span> * weights))

<span class="hljs-keyword">def</span> <span class="hljs-title function_">my_positive_weights</span>(<span class="hljs-params">weights</span>): <span class="hljs-comment"># return value is just tf.nn.relu(weights)</span>
    <span class="hljs-keyword">return</span> tf.where(weights &lt; <span class="hljs-number">0.</span>, tf.zeros_like(weights), weights)
</code></pre>
<p>可以看到，参数取决于自定义函数的类型。这些自定义函数可以如常使用，例如：</p>
<pre><code class="lang-py">layer = keras.layers.Dense(<span class="hljs-number">30</span>, activation=my_softplus,
                           kernel_initializer=my_glorot_initializer,
                           kernel_regularizer=my_l1_regularizer,
                           kernel_constraint=my_positive_weights)
</code></pre>
<p>激活函数会应用到这个<code>Dense</code>层的输出上，结果会传递到下一层。层的权重会使用初始化器的返回值。在每个训练步骤，权重会传递给正则化函数以计算正则损失，这个损失会与主损失相加，得到训练的最终损失。最后，会在每个训练步骤结束后调用约束函数，经过约束的权重会替换层的权重。</p>
<p>如果函数有需要连同模型一起保存的超参数，需要对相应的类做子类，比如<code>keras.regularizers.Regularizer</code>，<code>keras.constraints.Constraint</code>，<code>keras.initializers.Initializer</code>，或 <code>keras.layers.Layer</code>（任意层，包括激活函数）。就像前面的自定义损失一样，下面是一个简单的<code>ℓ1</code>正则类，可以保存它的超参数<code>factor</code>（这次不必调用其父构造器或<code>get_config()</code>方法，因为它们不是父类定义的）：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyL1Regularizer</span>(keras.regularizers.Regularizer):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, factor</span>):
        <span class="hljs-variable language_">self</span>.factor = factor
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, weights</span>):
        <span class="hljs-keyword">return</span> tf.reduce_sum(tf.<span class="hljs-built_in">abs</span>(<span class="hljs-variable language_">self</span>.factor * weights))
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_config</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> {<span class="hljs-string">"factor"</span>: <span class="hljs-variable language_">self</span>.factor}
</code></pre>
<p>注意，你必须要实现损失、层（包括激活函数）和模型的<code>call()</code>方法，或正则化器、初始化器和约束的<code>__call__()</code>方法。对于指标，处理方法有所不同。</p>
<h3 id="自定义指标">自定义指标</h3>
<p>损失和指标的概念是不一样的：梯度下降使用损失（比如交叉熵损失）来训练模型，因此损失必须是可微分的（至少是在评估点可微分），梯度不能在所有地方都是 0。另外，就算损失比较难解释也没有关系。相反的，指标（比如准确率）是用来评估模型的：指标的解释性一定要好，可以是不可微分的，或者可以在任何地方的梯度都是 0。</p>
<p>但是，在多数情况下，定义一个自定义指标函数和定义一个自定义损失函数是完全一样的。事实上，刚才创建的 Huber 损失函数也可以用来当指标（持久化也是同样的，只需要保存函数名<code>Huber_fn</code>就成）：</p>
<pre><code class="lang-py">model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">"mse"</span>, optimizer=<span class="hljs-string">"nadam"</span>, metrics=[create_huber(<span class="hljs-number">2.0</span>)])
</code></pre>
<p>对于训练中的每个批次，Keras 能计算该指标，并跟踪自周期开始的指标平均值。大多数时候，这样没有问题。但会有例外！比如，考虑一个二元分类器的准确性。第 3 章介绍过，准确率是真正值除以正预测数（包括真正值和假正值）。假设模型在第一个批次做了 5 个正预测，其中 4 个是正确的，准确率就是 80%。再假设模型在第二个批次做了 3 次正预测，但没有一个预测对，则准确率是 0%。如果对这两个准确率做平均，则平均值是 40%。但它不是模型在两个批次上的准确率！事实上，真正值总共有 4 个，正预测有 8 个，整体的准确率是 50%。我们需要的是一个能跟踪真正值和正预测数的对象，用该对象计算准确率。这就是类<code>keras.metrics.Precision</code>所做的：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>precision = keras.metrics.Precision()
<span class="hljs-meta">&gt;&gt;&gt; </span>precision([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">581729</span>, shape=(), dtype=float32, numpy=<span class="hljs-number">0.8</span>&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>precision([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>])
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">581780</span>, shape=(), dtype=float32, numpy=<span class="hljs-number">0.5</span>&gt;
</code></pre>
<p>在这个例子中，我们创建了一个<code>Precision</code>对象，然后将其用作函数，将第一个批次的标签和预测传给它，然后传第二个批次的数据（这里也可以传样本权重）。数据和前面的真正值和正预测一样。第一个批次之后，正确率是 80%；第二个批次之后，正确率是 50%（这是完整过程的准确率，不是第二个批次的准确率）。这叫做流式指标（或者静态指标），因为他是一个批次接一个批次，逐次更新的。</p>
<p>任何时候，可以调用<code>result()</code>方法获取指标的当前值。还可以通过<code>variables</code>属性，查看指标的变量（跟踪正预测和负预测的数量），还可以用<code>reset_states()</code>方法重置变量：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>p.result()
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">581794</span>, shape=(), dtype=float32, numpy=<span class="hljs-number">0.5</span>&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>p.variables
[&lt;tf.Variable <span class="hljs-string">'true_positives:0'</span> [...] numpy=array([<span class="hljs-number">4.</span>], dtype=float32)&gt;,
 &lt;tf.Variable <span class="hljs-string">'false_positives:0'</span> [...] numpy=array([<span class="hljs-number">4.</span>], dtype=float32)&gt;]
<span class="hljs-meta">&gt;&gt;&gt; </span>p.reset_states() <span class="hljs-comment"># both variables get reset to 0.0</span>
</code></pre>
<p>如果想创建一个这样的流式指标，可以创建一个<code>keras.metrics.Metric</code>类的子类。下面的例子跟踪了完整的 Huber 损失，以及实例的数量。当查询结果时，就能返回比例值，该值就是平均 Huber 损失：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">HuberMetric</span>(keras.metrics.Metric):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, threshold=<span class="hljs-number">1.0</span>, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(**kwargs) <span class="hljs-comment"># handles base args (e.g., dtype)</span>
        <span class="hljs-variable language_">self</span>.threshold = threshold
        <span class="hljs-variable language_">self</span>.huber_fn = create_huber(threshold)
        <span class="hljs-variable language_">self</span>.total = <span class="hljs-variable language_">self</span>.add_weight(<span class="hljs-string">"total"</span>, initializer=<span class="hljs-string">"zeros"</span>)
        <span class="hljs-variable language_">self</span>.count = <span class="hljs-variable language_">self</span>.add_weight(<span class="hljs-string">"count"</span>, initializer=<span class="hljs-string">"zeros"</span>)
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update_state</span>(<span class="hljs-params">self, y_true, y_pred, sample_weight=<span class="hljs-literal">None</span></span>):
        metric = <span class="hljs-variable language_">self</span>.huber_fn(y_true, y_pred)
        <span class="hljs-variable language_">self</span>.total.assign_add(tf.reduce_sum(metric))
        <span class="hljs-variable language_">self</span>.count.assign_add(tf.cast(tf.size(y_true), tf.float32))
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">result</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.total / <span class="hljs-variable language_">self</span>.count
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_config</span>(<span class="hljs-params">self</span>):
        base_config = <span class="hljs-built_in">super</span>().get_config()
        <span class="hljs-keyword">return</span> {**base_config, <span class="hljs-string">"threshold"</span>: <span class="hljs-variable language_">self</span>.threshold}
</code></pre>
<p>逐行看下代码：</p>
<ul>
<li><p>构造器使用<code>add_weight()</code>方法来创建用来跟踪多个批次的变量 —— 在这个例子中，就是 Huber 损失的和（<code>total</code>）和实例的数量（<code>count</code>）。如果愿意的话，可以手动创建变量。Keras 会跟中任何被设为属性的<code>tf.Variable</code>（更一般的讲，任何“可追踪对象”，比如层和模型）。</p>
</li>
<li><p>当将这个类的实例当做函数使用时会调用<code>update_state()</code>方法（正如<code>Precision</code>对象）。它能用每个批次的标签和预测值（还有样本权重，但这个例子忽略了样本权重）来更新变量。</p>
</li>
<li><p><code>result()</code>方法计算并返回最终值，在这个例子中，是返回所有实例的平均 Huber 损失。当你将指标用作函数时，<code>update_state()</code>方法先被调用，然后调用<code>result()</code>方法，最后返回输出。</p>
</li>
<li><p>还实现了<code>get_config()</code>方法，用以确保<code>threshold</code>和模型一起存储。</p>
</li>
<li><p><code>reset_states()</code>方法默认将所有值重置为 0.0（也可以改为其它值）。</p>
</li>
</ul>
<blockquote>
<p>笔记：Keras 能无缝处理变量持久化。</p>
</blockquote>
<p>当用简单函数定义指标时，Keras 会在每个批次自动调用它，还能跟踪平均值，就和刚才的手工处理一模一样。因此，<code>HuberMetric</code>类的唯一好处是<code>threshold</code>可以进行保存。当然，一些指标，比如准确率，不能简单的平均化；对于这些例子，只能实现一个流式指标。</p>
<p>创建好了流式指标，再创建自定义层就很简单了。</p>
<h3 id="自定义层">自定义层</h3>
<p>有时候你可能想搭建一个架构，但 TensorFlow 没有提供默认实现。这种情况下，就需要创建自定义层。否则只能搭建出的架构会是简单重复的，包含相同且重复的层块，每个层块实际上就是一个层而已。比如，如果模型的层顺序是 A、B、C、A、B、C、A、B、C，则完全可以创建一个包含 A、B、C 的自定义层 D，模型就可以简化为 D、D、D。</p>
<p>如何创建自定义层呢？首先，一些层没有权重，比如<code>keras.layers.Flatten</code>或<code>keras.layers.ReLU</code>。如果想创建一个没有任何权重的自定义层，最简单的方法是协议个函数，将其包装进<code>keras.layers.Lambda</code>层。比如，下面的层会对输入做指数运算：</p>
<pre><code class="lang-py">exponential_layer = keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.exp(x))
</code></pre>
<p>这个自定义层可以像任何其它层一样使用顺序 API、函数式 API 或子类化 API。你还可以将其用作激活函数（或者使用<code>activation=tf.exp</code>，<code>activation=keras.activations.exponential</code>，或者<code>activation="exponential"</code>）。当预测值的数量级不同时，指数层有时用在回归模型的输出层。</p>
<p>你可能猜到了，要创建自定义状态层（即，有权重的层），需要创建<code>keras.layers.Layer</code>类的子类。例如，下面的类实现了一个紧密层的简化版本：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDense</span>(keras.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, units, activation=<span class="hljs-literal">None</span>, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(**kwargs)
        <span class="hljs-variable language_">self</span>.units = units
        <span class="hljs-variable language_">self</span>.activation = keras.activations.get(activation)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self, batch_input_shape</span>):
        <span class="hljs-variable language_">self</span>.kernel = <span class="hljs-variable language_">self</span>.add_weight(
            name=<span class="hljs-string">"kernel"</span>, shape=[batch_input_shape[-<span class="hljs-number">1</span>], <span class="hljs-variable language_">self</span>.units],
            initializer=<span class="hljs-string">"glorot_normal"</span>)
        <span class="hljs-variable language_">self</span>.bias = <span class="hljs-variable language_">self</span>.add_weight(
            name=<span class="hljs-string">"bias"</span>, shape=[<span class="hljs-variable language_">self</span>.units], initializer=<span class="hljs-string">"zeros"</span>)
        <span class="hljs-built_in">super</span>().build(batch_input_shape) <span class="hljs-comment"># must be at the end</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, X</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.activation(X @ <span class="hljs-variable language_">self</span>.kernel + <span class="hljs-variable language_">self</span>.bias)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_output_shape</span>(<span class="hljs-params">self, batch_input_shape</span>):
        <span class="hljs-keyword">return</span> tf.TensorShape(batch_input_shape.as_list()[:-<span class="hljs-number">1</span>] + [<span class="hljs-variable language_">self</span>.units])

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_config</span>(<span class="hljs-params">self</span>):
        base_config = <span class="hljs-built_in">super</span>().get_config()
        <span class="hljs-keyword">return</span> {**base_config, <span class="hljs-string">"units"</span>: <span class="hljs-variable language_">self</span>.units,
                <span class="hljs-string">"activation"</span>: keras.activations.serialize(<span class="hljs-variable language_">self</span>.activation)}
</code></pre>
<p>逐行看下代码：</p>
<ul>
<li><p>构造器将所有超参数作为参数（这个例子中，是<code>units</code>和<code>activation</code>），更重要的，它还接收一个<code>**kwargs</code>参数。接着初始化了父类，传给父类<code>kwargs</code>：它负责标准参数，比如<code>input_shape</code>、<code>trainable</code>和<code>name</code>。然后将超参数存为属性，使用<code>keras.activations.get()</code>函数（这个函数接收函数、标准字符串，比如<code>"relu"</code>、<code>"selu"</code>、或<code>"None"</code>），将<code>activation</code>参数转换为合适的激活函数。</p>
</li>
<li><p><code>build()</code>方法通过对每个权重调用<code>add_weight()</code>方法，创建层的变量。层第一次被使用时，调用<code>build()</code>方法。此时，Keras 能知道该层输入的形状，并传入<code>build()</code>方法，这对创建权重是必要的。例如，需要知道前一层的神经元数量，来创建连接权重矩阵（即，<code>"kernel"</code>）：对应的是输入的最后一维的大小。在<code>build()</code>方法最后（也只是在最后），必须调用父类的<code>build()</code>方法：这步告诉 Keras 这个层建好了（或者设定<code>self.built=True</code>）。</p>
</li>
<li><p><code>call()</code>方法执行预想操作。在这个例子中，计算了输入<code>X</code>和层的核的矩阵乘法，加上了偏置向量，对结果使用了激活函数，得到了该层的输出。</p>
</li>
<li><p><code>compute_output_shape()</code>方法只是返回了该层输出的形状。在这个例子中，输出和输入的形状相同，除了最后一维被替换成了层的神经元数。在<code>tf.keras</code>中，形状是<code>tf.TensorShape</code>类的实例，可以用<code>as_list()</code>转换为 Python 列表。</p>
</li>
<li><p><code>get_config()</code>方法和前面的自定义类很像。注意是通过调用<code>keras.activations.serialize()</code>，保存了激活函数的完整配置。</p>
</li>
</ul>
<p>现在，就可以像其它层一样，使用<code>MyDense</code>层了！</p>
<blockquote>
<p>笔记：一般情况下，可以忽略<code>compute_output_shape()</code>方法，因为<code>tf.keras</code>能自动推断输出的形状，除非层是动态的（后面会看到动态层）。在其它 Keras 实现中，要么需要<code>compute_output_shape()</code>方法，要么默认输出形状和输入形状相同。</p>
</blockquote>
<p>要创建一个有多个输入（比如<code>Concatenate</code>）的层，<code>call()</code>方法的参数应该是包含所有输入的元组。相似的，<code>compute_output_shape()</code>方法的参数应该是一个包含每个输入的批次形状的元组。要创建一个有多输出的层，<code>call()</code>方法要返回输出的列表，<code>compute_output_shape()</code>方法要返回批次输出形状的列表（每个输出一个形状）。例如，下面的层有两个输入和三个输出：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyMultiLayer</span>(keras.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, X</span>):
        X1, X2 = X
        <span class="hljs-keyword">return</span> [X1 + X2, X1 * X2, X1 / X2]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_output_shape</span>(<span class="hljs-params">self, batch_input_shape</span>):
        b1, b2 = batch_input_shape
        <span class="hljs-keyword">return</span> [b1, b1, b1] <span class="hljs-comment"># 可能需要处理广播规则</span>
</code></pre>
<p>这个层现在就可以像其它层一样使用了，但只能使用函数式和子类化 API，顺序 API 不成（只能使用单输入和单输出的层）。</p>
<p>如果你的层需要在训练和测试时有不同的行为（比如，如果使用<code>Dropout</code> 或 <code>BatchNormalization</code>层），那么必须给<code>call()</code>方法加上<code>training</code>参数，用这个参数确定该做什么。比如，创建一个在训练中（为了正则）添加高斯造影的层，但不改动训练（Keras 有一个层做了同样的事，<code>keras.layers.GaussianNoise</code>）：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyGaussianNoise</span>(keras.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, stddev, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(**kwargs)
        <span class="hljs-variable language_">self</span>.stddev = stddev

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, X, training=<span class="hljs-literal">None</span></span>):
        <span class="hljs-keyword">if</span> training:
            noise = tf.random.normal(tf.shape(X), stddev=<span class="hljs-variable language_">self</span>.stddev)
            <span class="hljs-keyword">return</span> X + noise
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> X

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_output_shape</span>(<span class="hljs-params">self, batch_input_shape</span>):
        <span class="hljs-keyword">return</span> batch_input_shape
</code></pre>
<p>上面这些就能让你创建自定义层了！接下来看看如何创建自定义模型。</p>
<h3 id="自定义模型">自定义模型</h3>
<p>第 10 章在讨论子类化 API 时，接触过创建自定义模型的类。说白了：创建<code>keras.Model</code>类的子类，创建层和变量，用<code>call()</code>方法完成模型想做的任何事。假设你想搭建一个图 12-3 中的模型。</p>
<p><img src="img/17669b5f6b6e1a23b11d4207c365c15a.png" alt=""></img></p>
<p>图 12-3 自定义模型案例：包含残差块层，残块层含有跳连接</p>
<p>输入先进入一个紧密层，然后进入包含两个紧密层和一个添加操作的残差块（第 14 章会看见，残差块将输入和输出相加），经过 3 次同样的残差块，再通过第二个残差块，最终结果通过一个紧密输出层。这个模型没什么意义，只是一个搭建任意结构（包含循环和跳连接）模型的例子。要实现这个模型，最好先创建<code>ResidualBlock</code>层，因为这个层要用好几次：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResidualBlock</span>(keras.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_layers, n_neurons, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(**kwargs)
        <span class="hljs-variable language_">self</span>.hidden = [keras.layers.Dense(n_neurons, activation=<span class="hljs-string">"elu"</span>,
                                          kernel_initializer=<span class="hljs-string">"he_normal"</span>)
                       <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers)]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs</span>):
        Z = inputs
        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.hidden:
            Z = layer(Z)
        <span class="hljs-keyword">return</span> inputs + Z
</code></pre>
<p>这个层稍微有点特殊，因为它包含了其它层。用 Keras 来实现：自动检测<code>hidden</code>属性包含可追踪对象（即，层），内含层的变量可以自动添加到整层的变量列表中。类的其它部分很好懂。接下来，使用子类化 API 定义模型：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResidualRegressor</span>(keras.Model):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, output_dim, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(**kwargs)
        <span class="hljs-variable language_">self</span>.hidden1 = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">"elu"</span>,
                                          kernel_initializer=<span class="hljs-string">"he_normal"</span>)
        <span class="hljs-variable language_">self</span>.block1 = ResidualBlock(<span class="hljs-number">2</span>, <span class="hljs-number">30</span>)
        <span class="hljs-variable language_">self</span>.block2 = ResidualBlock(<span class="hljs-number">2</span>, <span class="hljs-number">30</span>)
        <span class="hljs-variable language_">self</span>.out = keras.layers.Dense(output_dim)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs</span>):
        Z = <span class="hljs-variable language_">self</span>.hidden1(inputs)
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span> + <span class="hljs-number">3</span>):
            Z = <span class="hljs-variable language_">self</span>.block1(Z)
        Z = <span class="hljs-variable language_">self</span>.block2(Z)
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.out(Z)
</code></pre>
<p>在构造器中创建层，在<code>call()</code>方法中使用。这个模型可以像其它模型那样来使用（编译、拟合、评估、预测）。如果你还想使用<code>save()</code>方法保存模型，使用<code>keras.models.load_model()</code>方法加载模型，则必须在<code>ResidualBlock</code>类和<code>ResidualRegressor</code>类中实现<code>get_config()</code>方法。另外，可以使用<code>save_weights()</code>方法和<code>load_weights()</code>方法保存和加载权重。</p>
<p><code>Model</code>类是<code>Layer</code>类的子类，因此模型可以像层一样定义和使用。但是模型还有一些其它的功能，包括<code>compile()</code>、<code>fit()</code>、<code>evaluate()</code> 和<code>predict()</code>（还有一些变量），还有<code>get_layers()</code>方法（它能通过名字或序号返回模型的任意层）、<code>save()</code>方法（支持<code>keras.models.load_model()</code>和<code>keras.models.clone_model()</code>）。</p>
<blockquote>
<p>提示：如果模型提供的功能比层多，为什么不讲每一个层定义为模型呢？技术上当然可以这么做，但对内部组件和模型（即，层或可重复使用的层块）加以区别，可以更加清晰。前者应该是<code>Layer</code>类的子类，后者应该是<code>Model</code>类的子类。</p>
</blockquote>
<p>掌握了上面的方法，你就可以使用顺序 API、函数式 API、子类化 API 搭建几乎任何文章上的模型了。为什么是“几乎”？因为还有些内容需要掌握：首先，如何基于模型内部定义损失或指标，第二，如何搭建自定义训练循环。</p>
<h3 id="基于模型内部的损失和指标">基于模型内部的损失和指标</h3>
<p>前面的自定义损失和指标都是基于标签和预测（或者还有样本权重）。有时，你可能想基于模型的其它部分定义损失，比如隐藏层的权重或激活函数。这么做，可以是处于正则的目的，或监督模型的内部。</p>
<p>要基于模型内部自定义损失，需要先做基于这些组件的计算，然后将结果传递给<code>add_loss()</code>方法。例如，自定义一个包含五个隐藏层加一个输出层的回归 MLP 模型。这个自定义模型基于上层的隐藏层，还有一个辅助的输出。和辅助输出关联的损失，被称为重建损失（见第 17 章）：它是重建和输入的均方差。通过将重建误差添加到主损失上，可以鼓励模型通过隐藏层保留尽量多的信息，即便是那些对回归任务没有直接帮助的信息。在实际中，重建损失有助于提高泛化能力（它是一个正则损失）。下面是含有自定义重建损失的自定义模型：</p>
<pre><code class="lang-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReconstructingRegressor</span>(keras.Model):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, output_dim, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(**kwargs)
        <span class="hljs-variable language_">self</span>.hidden = [keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">"selu"</span>,
                                          kernel_initializer=<span class="hljs-string">"lecun_normal"</span>)
                       <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)]
        <span class="hljs-variable language_">self</span>.out = keras.layers.Dense(output_dim)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self, batch_input_shape</span>):
        n_inputs = batch_input_shape[-<span class="hljs-number">1</span>]
        <span class="hljs-variable language_">self</span>.reconstruct = keras.layers.Dense(n_inputs)
        <span class="hljs-built_in">super</span>().build(batch_input_shape)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs</span>):
        Z = inputs
        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.hidden:
            Z = layer(Z)
        reconstruction = <span class="hljs-variable language_">self</span>.reconstruct(Z)
        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        <span class="hljs-variable language_">self</span>.add_loss(<span class="hljs-number">0.05</span> * recon_loss)
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.out(Z)
</code></pre>
<p>逐行看下代码：</p>
<ul>
<li><p>构造器搭建了一个有五个紧密层和一个紧密输出层的 DNN。</p>
</li>
<li><p><code>build()</code>方法创建了另一个紧密层，可以重建模型的输入。必须要在这里创建<code>build()</code>方法的原因，是单元的数量必须等于输入数，而输入数在调用<code>build()</code>方法之前是不知道的。</p>
</li>
<li><p><code>call()</code>方法处理所有五个隐藏层的输入，然后将结果传给重建层，重建层产生重建。</p>
</li>
<li><p><code>call()</code>方法然后计算重建损失（重建和输入的均方差），然后使用<code>add_loss()</code>方法，将其加到模型的损失列表上。注意，这里对重建损失乘以了 0.05（这是个可调节的超参数），做了缩小，以确保重建损失不主导主损失。</p>
</li>
<li><p>最后，<code>call()</code>方法将隐藏层的输出传递给输出层，然后返回输出。</p>
</li>
</ul>
<p>相似的，可以加上一个基于模型内部的自定义指标。例如，可以在构造器中创建一个<code>keras.metrics.Mean</code>对象，然后在<code>call()</code>方法中调用它，传递给它<code>recon_loss</code>，最后通过<code>add_metric()</code>方法，将其添加到模型上。使用这种方式，在训练模型时，Keras 能展示每个周期的平均损失（损失是主损失加上 0.05 乘以重建损失），和平均重建误差。两者都会在训练过程中下降：</p>
<pre><code class="lang-py">Epoch <span class="hljs-number">1</span>/<span class="hljs-number">5</span>
<span class="hljs-number">11610</span>/<span class="hljs-number">11610</span> [=============] [...] loss: <span class="hljs-number">4.3092</span> - reconstruction_error: <span class="hljs-number">1.7360</span>
Epoch <span class="hljs-number">2</span>/<span class="hljs-number">5</span>
<span class="hljs-number">11610</span>/<span class="hljs-number">11610</span> [=============] [...] loss: <span class="hljs-number">1.1232</span> - reconstruction_error: <span class="hljs-number">0.8964</span>
[...]
</code></pre>
<p>在超过 99% 的情况中，前面所讨论的内容已经足够搭建你想要的模型了，就算是包含复杂架构、损失和指标也行。但是，在某些极端情况，你还需要自定义训练循环。介绍之前，先来看看 TensorFlow 如何自动计算梯度。</p>
<h3 id="使用自动微分计算梯度">使用自动微分计算梯度</h3>
<p>要搞懂如何使用自动微分自动计算梯度，来看一个例子：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">w1, w2</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-number">3</span> * w1 ** <span class="hljs-number">2</span> + <span class="hljs-number">2</span> * w1 * w2
</code></pre>
<p>如果你会微积分，就能算出这个函数对<code>w1</code>的偏导是<code>6 * w1 + 2 * w2</code>，还能算出它对<code>w2</code>的偏导是<code>2 * w1</code>。例如，在点<code>(w1, w2) = (5, 3)</code>，这两个偏导数分别是 36 和 10，在这个点的梯度向量就是<code>(36, 10)</code>。但对于神经网络来说，函数会复杂得多，可能会有上完个参数，用手算偏导几乎是不可能的任务。一个解决方法是计算每个偏导的大概值，通过调节参数，查看输出的变化：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>w1, w2 = <span class="hljs-number">5</span>, <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>eps = <span class="hljs-number">1e-6</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>(f(w1 + eps, w2) - f(w1, w2)) / eps
<span class="hljs-number">36.000003007075065</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>(f(w1, w2 + eps) - f(w1, w2)) / eps
<span class="hljs-number">10.000000003174137</span>
</code></pre>
<p>这种方法很容易实现，但只是大概。重要的是，需要对每个参数至少要调用一次<code>f()</code>（不是至少两次，因为可以只计算一次<code>f(w1, w2)</code>）。这样，对于大神经网络，就不怎么可控。所以，应该使用自动微分。TensorFlow 的实现很简单：</p>
<pre><code class="lang-py">w1, w2 = tf.Variable(<span class="hljs-number">5.</span>), tf.Variable(<span class="hljs-number">3.</span>)
<span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
    z = f(w1, w2)

gradients = tape.gradient(z, [w1, w2])
</code></pre>
<p>先定义了两个变量<code>w1</code> 和 <code>w2</code>，然后创建了一个<code>tf.GradientTape</code>上下文，它能自动记录变脸的每个操作，最后使用它算出结果<code>z</code>关于两个变量<code>[w1, w2]</code>的梯度。TensorFlow 计算的梯度如下：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>gradients
[&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">828234</span>, shape=(), dtype=float32, numpy=<span class="hljs-number">36.0</span>&gt;,
 &lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">828229</span>, shape=(), dtype=float32, numpy=<span class="hljs-number">10.0</span>&gt;]
</code></pre>
<p>很好！不仅结果是正确的（准确度只受浮点误差限制），<code>gradient()</code>方法只逆向算了一次，无论有多少个变量，效率很高。</p>
<blockquote>
<p>提示：为了节省内存，只将严格的最小值放在<code>tf.GradientTape()</code>中。另外，通过<code>在 tf.GradientTape()</code>中创建一个<code>tape.stop_recording()</code>来暂停记录。</p>
</blockquote>
<p>当调用记录器的<code>gradient()</code>方法时，记录器会自动清零，所以调用两次<code>gradient()</code>就会报错：</p>
<pre><code class="lang-py"><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
    z = f(w1, w2)

dz_dw1 = tape.gradient(z, w1) <span class="hljs-comment"># =&gt; tensor 36.0</span>
dz_dw2 = tape.gradient(z, w2) <span class="hljs-comment"># 运行时错误</span>
</code></pre>
<p>如果需要调用<code>gradient()</code>一次以上，比续将记录器持久化，并在每次用完之后删除，释放资源：</p>
<pre><code class="lang-py"><span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> tape:
    z = f(w1, w2)

dz_dw1 = tape.gradient(z, w1) <span class="hljs-comment"># =&gt; tensor 36.0</span>
dz_dw2 = tape.gradient(z, w2) <span class="hljs-comment"># =&gt; tensor 10.0, works fine now!</span>
<span class="hljs-keyword">del</span> tape
</code></pre>
<p>默认情况下，记录器只会跟踪包含变量的操作，所以如果是计算<code>z</code>的梯度，<code>z</code>和变量没关系，结果就会是<code>None</code>：</p>
<pre><code class="lang-py">c1, c2 = tf.constant(<span class="hljs-number">5.</span>), tf.constant(<span class="hljs-number">3.</span>)
<span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
    z = f(c1, c2)

gradients = tape.gradient(z, [c1, c2]) <span class="hljs-comment"># returns [None, None]</span>
</code></pre>
<p>但是，你也可以强制记录器监视任何你想监视的张量，将它们当做变量来计算梯度：</p>
<pre><code class="lang-py"><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
    tape.watch(c1)
    tape.watch(c2)
    z = f(c1, c2)

gradients = tape.gradient(z, [c1, c2]) <span class="hljs-comment"># returns [tensor 36., tensor 10.]</span>
</code></pre>
<p>在某些情况下，这么做会有帮助，比如当输入的波动很小，而激活函数结果波动很大时，要实现一个正则损失，就可以这么做：损失会基于激活函数结果，激活函数结果会基于输入。因为输入不是变量，就需要记录器监视输入。</p>
<p>大多数时候，梯度记录器被用来计算单一值（通常是损失）的梯度。这就是自动微分发挥长度的地方了。因为自动微分只需要一次向前传播一次向后传播，就能计算所有梯度。如果你想计算一个向量的梯度，比如一个包含多个损失的向量，TensorFlow 就会计算向量和的梯度。因此，如果你需要计算单个梯度的话（比如每个损失相对于模型参数的梯度），你必须调用记录器的<code>jabobian()</code>方法：它能做反向模式的自动微分，一次计算完向量中的所有损失（默认是并行的）。甚至还可以计算二级偏导，但在实际中用的不多（见笔记本中的“自动微分计算梯度部分”）。</p>
<p>某些情况下，你可能想让梯度在部分神经网络停止传播。要这么做的话，必须使用<code>tf.stop_gradient()</code>函数。它能在前向传播中（比如<code>tf.identity()</code>）返回输入，并能阻止梯度反向传播（就像常量一样）：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">w1, w2</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-number">3</span> * w1 ** <span class="hljs-number">2</span> + tf.stop_gradient(<span class="hljs-number">2</span> * w1 * w2)

<span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
    z = f(w1, w2) <span class="hljs-comment"># same result as without stop_gradient()</span>

gradients = tape.gradient(z, [w1, w2]) <span class="hljs-comment"># =&gt; returns [tensor 30., None]</span>
</code></pre>
<p>最后，在计算梯度时可能还会碰到数值问题。例如，如果对于很大的输入，计算<code>my_softplus()</code>函数的梯度，结果会是<code>NaN</code>：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = tf.Variable([<span class="hljs-number">100.</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
<span class="hljs-meta">... </span>    z = my_softplus(x)
...
<span class="hljs-meta">&gt;&gt;&gt; </span>tape.gradient(z, [x])
&lt;tf.Tensor: [...] numpy=array([nan], dtype=float32)&gt;
</code></pre>
<p>这是因为使用自动微分计算这个函数的梯度，会有些数值方面的难点：因为浮点数的精度误差，自动微分最后会变成无穷除以无穷（结果是<code>NaN</code>）。幸好，softplus 函数的导数是<code>1 / (1 + 1 / exp(x))</code>，它是数值稳定的。接着，让 TensorFlow 使用这个稳定的函数，通过装饰器<code>@tf.custom_gradient</code>计算<code>my_softplus()</code>的梯度，既返回正常输出，也返回计算导数的函数（注意：它会接收的输入是反向传播的梯度；根据链式规则，应该乘以函数的梯度）：</p>
<pre><code class="lang-py"><span class="hljs-meta">@tf.custom_gradient</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">my_better_softplus</span>(<span class="hljs-params">z</span>):
    exp = tf.exp(z)
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">my_softplus_gradients</span>(<span class="hljs-params">grad</span>):
        <span class="hljs-keyword">return</span> grad / (<span class="hljs-number">1</span> + <span class="hljs-number">1</span> / exp)
    <span class="hljs-keyword">return</span> tf.math.log(exp + <span class="hljs-number">1</span>), my_softplus_gradients
</code></pre>
<p>计算好了<code>my_better_softplus()</code>的梯度，就算对于特别大的输入值，也能得到正确的结果（但是，因为指数运算，主输出还是会发生爆炸；绕过的方法是，当输出很大时，使用<code>tf.where()</code>返回输入）。</p>
<p>祝贺你！现在你就可以计算任何函数的梯度（只要函数在计算点可微就行），甚至可以阻止反向传播，还能写自己的梯度函数！TensorFlow 的灵活性还能让你编写自定义的训练循环。</p>
<h3 id="自定义训练循环">自定义训练循环</h3>
<p>在某些特殊情况下，<code>fit()</code>方法可能不够灵活。例如，第 10 章讨论过的 Wide &amp; Deep 论文使用了两个优化器：一个用于宽路线，一个用于深路线。因为<code>fit()</code>方法智能使用一个优化器（编译时设置的优化器），要实现这篇论文就需要写自定义循环。</p>
<p>你可能还想写自定义的训练循环，只是想让训练过程更加可控（也许你对<code>fit()</code>方法的细节并不确定）。但是，自定义训练循环会让代码变长、更容易出错、也难以维护。</p>
<blockquote>
<p>提示：除非真的需要自定义，最好还是使用<code>fit()</code>方法，而不是自定义训练循环，特别是当你是在一个团队之中时。</p>
</blockquote>
<p>首先，搭建一个简单的模型。不用编译，因为是要手动处理训练循环：</p>
<pre><code class="lang-py">l2_reg = keras.regularizers.l2(<span class="hljs-number">0.05</span>)
model = keras.models.Sequential([
    keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">"elu"</span>, kernel_initializer=<span class="hljs-string">"he_normal"</span>,
                       kernel_regularizer=l2_reg),
    keras.layers.Dense(<span class="hljs-number">1</span>, kernel_regularizer=l2_reg)
])
</code></pre>
<p>接着，创建一个小函数，它能从训练集随机采样一个批次的实例（第 13 章会讨论更便捷的 Data API）：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_batch</span>(<span class="hljs-params">X, y, batch_size=<span class="hljs-number">32</span></span>):
    idx = np.random.randint(<span class="hljs-built_in">len</span>(X), size=batch_size)
    <span class="hljs-keyword">return</span> X[idx], y[idx]
</code></pre>
<p>再定义一个可以展示训练状态的函数，包括步骤数、总步骤数、平均损失（用<code>Mean</code>指标计算），和其它指标：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_status_bar</span>(<span class="hljs-params">iteration, total, loss, metrics=<span class="hljs-literal">None</span></span>):
    metrics = <span class="hljs-string">" - "</span>.join([<span class="hljs-string">"{}: {:.4f}"</span>.<span class="hljs-built_in">format</span>(m.name, m.result())
                         <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> [loss] + (metrics <span class="hljs-keyword">or</span> [])])
    end = <span class="hljs-string">""</span> <span class="hljs-keyword">if</span> iteration &lt; total <span class="hljs-keyword">else</span> <span class="hljs-string">"\n"</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"\r{}/{} - "</span>.<span class="hljs-built_in">format</span>(iteration, total) + metrics,
          end=end)
</code></pre>
<p>这段代码不难，除非你对 Python 字符串的<code>{:.4f}</code>不熟：它的作用是保留四位小数。使用<code>\r</code>（回车）和<code>end=""</code>连用，保证状态条总是打印在一条线上。笔记本中，<code>print_status_bar()</code>函数包括进度条，也可以使用<code>tqdm</code>库。</p>
<p>有了这些准备，就可以开干了！首先，我们定义超参数、选择优化器、损失函数和指标（这个例子中是 MAE）：</p>
<pre><code class="lang-py">n_epochs = <span class="hljs-number">5</span>
batch_size = <span class="hljs-number">32</span>
n_steps = <span class="hljs-built_in">len</span>(X_train) // batch_size
optimizer = keras.optimizers.Nadam(lr=<span class="hljs-number">0.01</span>)
loss_fn = keras.losses.mean_squared_error
mean_loss = keras.metrics.Mean()
metrics = [keras.metrics.MeanAbsoluteError()]
</code></pre>
<p>可以搭建自定义循环了：</p>
<pre><code class="lang-py"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_epochs + <span class="hljs-number">1</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Epoch {}/{}"</span>.<span class="hljs-built_in">format</span>(epoch, n_epochs))
    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_steps + <span class="hljs-number">1</span>):
        X_batch, y_batch = random_batch(X_train_scaled, y_train)
        <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
            y_pred = model(X_batch, training=<span class="hljs-literal">True</span>)
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
            loss = tf.add_n([main_loss] + model.losses)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, model.trainable_variables))
        mean_loss(loss)
        <span class="hljs-keyword">for</span> metric <span class="hljs-keyword">in</span> metrics:
            metric(y_batch, y_pred)
        print_status_bar(step * batch_size, <span class="hljs-built_in">len</span>(y_train), mean_loss, metrics)
    print_status_bar(<span class="hljs-built_in">len</span>(y_train), <span class="hljs-built_in">len</span>(y_train), mean_loss, metrics)
    <span class="hljs-keyword">for</span> metric <span class="hljs-keyword">in</span> [mean_loss] + metrics:
        metric.reset_states()
</code></pre>
<p>逐行看下代码：</p>
<ul>
<li><p>创建了两个嵌套循环：一个是给周期的，一个是给周期里面的批次的。</p>
</li>
<li><p>然后从训练集随机批次采样。</p>
</li>
<li><p>在<code>tf.GradientTape()</code>内部，对一个批次做了预测（将模型用作函数），计算其损失：损失等于主损失加上其它损失（在这个模型中，每层有一个正则损失）。因为<code>mean_squared_error()</code>函数给每个实例返回一个损失，使用<code>tf.reduce_mean()</code>计算平均值（如果愿意的话，每个实例可以用不同的权重）。正则损失已经转变为单个的标量，所以只需求和就成（使用<code>tf.add_n()</code>，它能将相同形状和数据类型的张量求和）。</p>
</li>
<li><p>接着，让记录器计算损失相对于每个可训练变量的梯度（不是所有的变量！），然后用优化器对梯度做梯度下降。</p>
</li>
<li><p>然后，更新（当前周期）平均损失和平均指标，显示状态条。</p>
</li>
<li><p>在每个周期结束后，再次展示状态条，使其完整，然后换行，重置平均损失和平均指标。</p>
</li>
</ul>
<p>如果设定优化器的<code>clipnorm</code>或<code>clipvalue</code>超参数，就可以自动重置。如果你想对梯度做任何其它变换，在调用<code>apply_gradients()</code>方法之前，做变换就行。</p>
<p>如果你对模型添加了权重约束（例如，添加层时设置<code>kernel_constraint</code>或<code>bias_constraint</code>），你需要在<code>apply_gradients()</code>之后，更新训练循环，以应用这些约束：</p>
<pre><code class="lang-py"><span class="hljs-keyword">for</span> variable <span class="hljs-keyword">in</span> model.variables:
    <span class="hljs-keyword">if</span> variable.constraint <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        variable.assign(variable.constraint(variable))
</code></pre>
<p>最重要的，这个训练循环没有处理训练和测试过程中，行为不一样的层（例如，<code>BatchNormalization</code>或<code>Dropout</code>）。要处理的话，需要调用模型，令<code>training=True</code>，并传播到需要这么设置的每一层。</p>
<p>可以看到，有这么多步骤都要做对才成，很容易出错。但另一方面，训练的控制权完全在你手里。</p>
<p>现在你知道如何自定义模型中的任何部分了，也知道如何训练算法了，接下来看看如何使用 TensorFlow 的自动图生成特征：它能显著提高自定义代码的速度，并且还是可迁移的（见第 19 章）。</p>
<h2 id="tensorflow-的函数和图">TensorFlow 的函数和图</h2>
<p>在 TensorFlow 1 中，图是绕不过去的（同时图也很复杂），因为图是 TensorFlow 的 API 的核心。在 TensorFlow 2 中，图还在，但不是核心了，使用也简单多了。为了演示其易用性，从一个三次方函数开始：</p>
<pre><code class="lang-py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cube</span>(<span class="hljs-params">x</span>):
    <span class="hljs-keyword">return</span> x ** <span class="hljs-number">3</span>
</code></pre>
<p>可以用一个值调用这个函数，整数、浮点数都成，或者用张量来调用：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>cube(<span class="hljs-number">2</span>)
<span class="hljs-number">8</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>cube(tf.constant(<span class="hljs-number">2.0</span>))
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">18634148</span>, shape=(), dtype=float32, numpy=<span class="hljs-number">8.0</span>&gt;
</code></pre>
<p>现在，使用<code>tf.function()</code>将这个 Python 函数变为 TensorFlow 函数：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>tf_cube = tf.function(cube)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_cube
&lt;tensorflow.python.eager.def_function.Function at <span class="hljs-number">0x1546fc080</span>&gt;
</code></pre>
<p>可以像原生 Python 函数一样使用这个 TF 函数，可以返回同样的结果（张量）：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>tf_cube(<span class="hljs-number">2</span>)
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">18634201</span>, shape=(), dtype=int32, numpy=<span class="hljs-number">8</span>&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_cube(tf.constant(<span class="hljs-number">2.0</span>))
&lt;tf.Tensor: <span class="hljs-built_in">id</span>=<span class="hljs-number">18634211</span>, shape=(), dtype=float32, numpy=<span class="hljs-number">8.0</span>&gt;
</code></pre>
<p><code>tf.function()</code>在底层分析了<code>cube()</code>函数的计算，然后生成了一个等价的计算图！可以看到，过程十分简单（下面会讲解过程）。另外，也可以使用<code>tf.function</code>作为装饰器，更常见一些：</p>
<pre><code class="lang-py"><span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">tf_cube</span>(<span class="hljs-params">x</span>):
    <span class="hljs-keyword">return</span> x ** <span class="hljs-number">3</span>
</code></pre>
<p>原生的 Python 函数通过 TF 函数的<code>python_function</code>属性仍然可用：</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>tf_cube.python_function(<span class="hljs-number">2</span>)
<span class="hljs-number">8</span>
</code></pre>
<p>TensorFlow 优化了计算图，删掉了没用的节点，简化了表达式（比如，<code>1 + 2</code>会替换为 3），等等。当优化好的计算图准备好之后，TF 函数可以在图中，按合适的顺序高效执行运算（该并行的时候就并行）。作为结果，TF 函数比普通的 Python 函数快的做，特别是在做复杂计算时。大多数时候，根本没必要知道底层到底发生了什么，如果需要对 Python 函数加速，将其转换为 TF 函数就行。</p>
<p>另外，当你写的自定义损失函数、自定义指标、自定义层或任何其它自定义函数，并在 Keras 模型中使用的，Keras 都自动将其转换成了 TF 函数，不用使用<code>tf.function()</code>。</p>
<blockquote>
<p>提示：创建自定义层或模型时，设置<code>dynamic=True</code>，可以让 Keras 不转化你的 Python 函数。另外，当调用模型的<code>compile()</code>方法时，可以设置<code>run_eagerly=True</code>。</p>
</blockquote>
<p>默认时，TF 函数对每个独立输入的形状和数据类型的集合，生成了一个新的计算图，并缓存以备后续使用。例如，如果你调用<code>tf_cube(tf.constant(10))</code>，就会生成一个<code>int32</code>张量、形状是[]的计算图。如果你调用<code>tf_cube(tf.constant(20))</code>，会使用相同的计算图。但如果调用<code>tf_cube(tf.constant([10, 20]))</code>，就会生成一个<code>int32</code>、形状是<code>[2]</code>的新计算图。这就是 TF 如何处理多态的（即变化的参数类型和形状）。但是，这只适用于张量参数：如果你将 Python 数值传给 TF，就会为每个独立值创建一个计算图：比如，调用<code>tf_cube(10)</code>和<code>tf_cube(20)</code>会产生两个计算图。</p>
<blockquote>
<p>警告：如果用多个不同的 Python 数值调用 TF 函数，就会产生多个计算图，这样会减慢程勋，使用很多的内存（必须删掉 TF 函数才能释放）。Python 的值应该复赋值给尽量重复的参数，比如超参数，每层有多少个神经元。这可以让 TensorFlow 更好的优化模型中的变量。</p>
</blockquote>
<h3 id="自动图和跟踪">自动图和跟踪</h3>
<p>TensorFlow 是如何生成计算图的呢？它先分析了 Python 函数源码，得出所有的数据流控制语句，比如<code>for</code>循环，<code>while</code>循环，<code>if</code>条件，还有<code>break</code>、<code>continue</code>、<code>return</code>。这个第一步被称为自动图（AutoGraph）。TensorFlow 之所以要分析源码，试分析 Python 没有提供任何其它的方式来获取控制流语句：Python 提供了<code>__add__()</code>和<code>__mul__()</code>这样的魔术方法，但没有<code>__while__()</code>或<code>__if__()</code>这样的魔术方法。分析完源码之后，自动图中的所有控制流语句都被替换成相应的 TensorFlow 方法，比如<code>tf.while_loop()</code>（<code>while</code>循环）和<code>tf.cond()</code>（<code>if</code>判断）。例如，见图 12-4，自动图分析了 Python 函数<code>sum_squares()</code>的源码，然后变为函数<code>tf__sum_squares()</code>。在这个函数中，<code>for</code>循环被替换成了<code>loop_body()</code>（包括原生的<code>for</code>循环）。然后是函数<code>for_stmt()</code>，调用这个函数会形成运算<code>tf.while_loop()</code>。</p>
<p><img src="img/69bc4daad92e36575011ff9abc5c3148.png" alt=""></img></p>
<p>图 12-4 TensorFlow 是如何使用自动图和跟踪生成计算图的？</p>
<p>然后，TensorFlow 调用这个“升级”方法，但没有向其传递参数，而是传递一个符号张量（symbolic tensor）——一个没有任何真实值的张量，只有名字、数据类型和形状。例如，如果调用<code>sum_squares(tf.constant(10))</code>，然后会调用<code>tf__sum_squares()</code>，其符号张量的类型是<code>int32</code>，形状是<code>[]</code>。函数会以图模式运行，意味着每个 TensorFlow 运算会在图中添加一个表示自身的节点，然后输出<code>tensor(s)</code>（与常规模式相对，这被称为动态图执行，或动态模式）。在图模式中，TF 运算不做任何计算。如果你懂 TensorFlow 1，这应该很熟悉，因为图模式是默认模式。在图 12-4 中，可以看到<code>tf__sum_squares()</code>函数被调用，参数是符号张量，最后的图是跟踪中生成的。节点表示运算，箭头表示张量（生成的函数和图都简化了）。</p>
<blockquote>
<p>提示：想看生成出来的函数源码的话，可以调用<code>tf.autograph.to_code(sum_squares.python_function)</code>。源码不美观，但可以用来调试。</p>
</blockquote>
<h3 id="tf-函数规则">TF 函数规则</h3>
<p>大多数时候，将 Python 函数转换为 TF 函数是琐碎的：要用<code>@tf.function</code>装饰，或让 Keras 来负责。但是，也有一些规则：</p>
<ul>
<li><p>如果调用任何外部库，包括 NumPy，甚至是标准库，调用只会在跟踪中运行，不会是图的一部分。事实上，TensorFlow 图只能包括 TensorFlow 的构件（张量、运算、变量、数据集，等等）。因此，要确保使用的是<code>tf.reduce_sum()</code>而不是<code>np.sum()</code>，使用的是<code>tf.sort()</code>而不是内置的<code>sorted()</code>，等等。还要注意：</p>
</li>
<li><p>如果定义了一个 TF 函数<code>f(x)</code>，它只返回<code>np.random.rand()</code>，当函数被追踪时，生成的是个随机数，因此<code>f(tf.constant(2.))</code>和<code>f(tf.constant(3.))</code>会返回同样的随机数，但<code>f(tf.constant([2., 3.]))</code>会返回不同的数。如果将<code>np.random.rand()</code>替换为<code>tf.random.uniform([])</code>，每次调用都会返回新的随机数，因为运算是图的一部分。</p>
</li>
<li><p>如果你的非 TensorFlow 代码有副作用（比如日志，或更新 Python 计数器），则 TF 函数被调用时，副作用不一定发生，因为只有函数被追踪时才有效。</p>
</li>
<li><p>你可以在<code>tf.py_function()</code>运算中包装任意的 Python 代码，但这么做的话会使性能下降，因为 TensorFlow 不能做任何图优化。还会破坏移植性，因为图只能在有 Python 的平台上跑起来（且安装上正确的库）。</p>
</li>
<li><p>你可以调用其它 Python 函数或 TF 函数，但是它们要遵守相同的规则，因为 TensorFlow 会在计算图中记录它们的运算。注意，其它函数不需要用<code>@tf.function</code>装饰。</p>
</li>
<li><p>如果函数创建了一个 TensorFlow 变量（或任意其它静态 TensorFlow 对象，比如数据集或队列），它必须在第一次被调用时创建 TF 函数，否则会导致异常。通常，最好在 TF 函数的外部创建变量（比如在自定义层的<code>build()</code>方法中）。如果你想将一个新值赋值给变量，要确保调用它的<code>assign()</code>方法，而不是使用<code>=</code>。</p>
</li>
<li><p>Python 的源码可以被 TensorFlow 使用。如果源码用不了（比如，如果是在 Python shell 中定义函数，源码就访问不了，或者部署的是编译文件<code>*.pyc</code>），图的生成就会失败或者缺失功能。</p>
</li>
<li><p>TensorFlow 只能捕获迭代张量或数据集的<code>for</code>循环。因此要确保使用<code>for i in tf.range(x)</code>，而不是<code>for i in range(x)</code>，否则循环不能在图中捕获，而是在会在追踪中运行。（如果<code>for</code>循环使用创建计算图的，这可能是你想要的，比如创建神经网络中的每一层）。</p>
</li>
<li><p>出于性能原因，最好使用向量化的实现方式，而不是使用循环。</p>
</li>
</ul>
<p>总结一下，这一章一开始介绍了 TensorFlow，然后是 TensorFlow 的低级 API，包括张量、运算、变量和特殊的数据结构。然后使用这些工具自定义了<code>tf.keras</code>中的几乎每个组件。最后，学习了 TF 函数如何提升性能，计算图是如何通过自动图和追踪生成的，在写 TF 函数时要遵守什么规则。（附录 G 介绍了生成图的内部黑箱）</p>
<p>下一章会学习如何使用 TensorFlow 高效加载和预处理数据。</p>
<h1 id="练习">练习</h1>
<ol>
<li><p>如何用一句话描述 TensorFlow？它的主要特点是什么？能列举出其它流行的深度学习库吗？</p>
</li>
<li><p>TensorFlow 是 NumPy 的简单替换吗？二者有什么区别？</p>
</li>
<li><p><code>tf.range(10)</code>和<code>tf.constant(np.arange(10))</code>能拿到相同的结果吗？</p>
</li>
<li><p>列举出除了常规张量之外，TensorFlow 的其它六种数据结构？</p>
</li>
<li><p>可以通过函数或创建<code>keras.losses.Loss</code>的子类来自定义损失函数。两种方法各在什么时候使用？</p>
</li>
<li><p>相似的，自定义指标可以通过定义函数或创建<code>keras.metrics.Metric</code>的子类。两种方法各在什么时候使用？</p>
</li>
<li><p>什么时候应该创建自定义层，而不是自定义模型？</p>
</li>
<li><p>什么时候需要创建自定义的训练循环？</p>
</li>
<li><p>自定义 Keras 组件可以包含任意 Python 代码吗，或者 Python 代码需要转换为 TF 函数吗？</p>
</li>
<li><p>如果想让一个函数可以转换为 TF 函数，要遵守设么规则？</p>
</li>
<li><p>什么时候需要创建一个动态 Keras 模型？怎么做？为什么不让所有模型都是动态的？</p>
</li>
<li><p>实现一个具有层归一化的自定义层（第 15 章会用到）：</p>
</li>
</ol>
<p>a. <code>build()</code>方法要定义两个可训练权重<code>α</code>和<code>β</code>，形状都是<code>input_shape[-1:]</code>，数据类型是<code>tf.float32</code>。<code>α</code>用 1 初始化，<code>β</code>用 0 初始化。</p>
<p>b. <code>call()</code>方法要计算每个实例的特征的平均值<code>μ</code>和标准差<code>σ</code>。你可以使用<code>tf.nn.moments(inputs, axes=-1, keepdims=True)</code>，它可以返回平均值<code>μ</code>和方差<code>σ^2</code>（计算其平方根得到标准差）。函数返回<code>α⊗(X - μ)/(σ + ε) + β</code>，其中<code>⊗</code>表示元素级别惩罚，<code>ε</code>是平滑项（避免发生除以 0，而是除以 0.001）。</p>
<p>c. 确保自定义层的输出和<code>keras.layers.LayerNormalization</code>层的输出一致（或非常接近）。</p>
<ol>
<li>训练一个自定义训练循环，来处理 Fashion MNIST 数据集。</li>
</ol>
<p>a. 展示周期、迭代，每个周期的平均训练损失、平均准确度（每次迭代会更新），还有每个周期结束后的验证集损失和准确度。</p>
<p>b. 深层和浅层使用不同的优化器，不同的学习率。</p>
<p>参考答案见附录 A。</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="11.html" class="navigation navigation-prev " aria-label="Previous page: 十一、训练深度神经网络">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="13.html" class="navigation navigation-next " aria-label="Next page: 十三、使用 TensorFlow 加载和预处理数据">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"十二、使用 TensorFlow 自定义模型并训练","level":"1.13","depth":1,"next":{"title":"十三、使用 TensorFlow 加载和预处理数据","level":"1.14","depth":1,"path":"13.md","ref":"./13.md","articles":[]},"previous":{"title":"十一、训练深度神经网络","level":"1.12","depth":1,"path":"11.md","ref":"./11.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"12.md","mtime":"2024-08-23T21:05:56.811Z","type":"markdown"},"gitbook":{"version":"5.1.4","time":"2024-08-24T12:29:35.670Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

